<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 1: Eigenvectors and Eigenvalues</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">Before we dive into more linear algebra, I want to take a step back and look at some of its many applications that motivate devoting two terms to its study. Now that we have the terminology of bases, the definition of a linear map <span class="tex-holder inline-math" data-source-tex="T : V \to W">$T : V \to W$</span> couldn&#x2019;t be much simpler: it&#x2019;s a function that splits across addition and scalar multiplication (i.e. <span class="tex-holder inline-math" data-source-tex="T(c\vec{v_1} + \vec{v_2}) = cT(\vec{v_1}) + T(\vec{v_2})">$T(c\vec{v_1} + \vec{v_2}) = cT(\vec{v_1}) + T(\vec{v_2})$)</span> that&#x2019;s completely determined by its action on a basis.</p><p class="body-text">With <span class="tex-holder inline-math" data-source-tex="V = \mathbb{R}^3">$V = \mathbb{R}^3$</span> and <span class="tex-holder inline-math" data-source-tex="W = \mathbb{R}^2">$W = \mathbb{R}^2$,</span> there&#x2019;s the classic example of 3D graphics. <em>Raster</em> graphics, the technology underlying most 3D video games, works by modeling 3D shapes as a massive collection of triangles, then projecting each onto the 2D screen by a linear map determined by the position and rotation of the camera. The projection of a 3D triangle is a 2D triangle (this takes a little thought!), and so the result is a collection of triangles in the image plane. We can then test every pixel to see if it&#x2019;s in the interior of a triangle, and color it accordingly if so. At the time of writing, there&#x2019;s no Desmos 3D API I can use to embed a graph directly in the notes, but here&#x2019;s <a href="https://www.desmos.com/3d/947d6c19bf">a small demonstration I put together</a>. The gray lines connecting the vertices of the red triangle in the world to its blue rendered image are projections from the ambient space onto the camera&#x2019;s image plane.</p><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: 3D rendering</div><p class="body-text">In the previous example, what would be a good choice of bases for the domain and codomain of the projection map to make its matrix representation as simple as possible? Explain your answer &mdash; it doesn&#x2019;t need to be in symbols.</p></div><p class="body-text">On the subject of 3D rendering, I completed a project to render the Thurston geometries not too long ago. These are the eight different geometries possible in curved three-dimensional space; as a two-dimensional analogue, the geometry of the sphere is different than that of the plane. Here&#x2019;s a rendering of three-dimensional <em>hyperbolic</em> space &mdash; drag on the scene to look around and use WASD or hold down with two fingers on a touchscreen to move.</p><div class="desmos-border canvas-container"><canvas id="h3-geometry-canvas" class="output-canvas"></canvas></div><p class="body-text">Linear algebra is all over this project. All of the geometries are represented as curved spaces in 4-dimensional space, similar to how a sphere is curved in 3-dimensional space, and so the facing of the camera is stored as four vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> that are all perpendicular to one another. Although the scene itself looks like an infinite collection of rooms, there&#x2019;s actually only a single one. The camera casts a ray out to render each pixel, and whenever it passes through one of the room&#x2019;s windows, a linear map (i.e. a <span class="tex-holder inline-math" data-source-tex="4 \times 4">$4 \times 4$</span> matrix) gets applied to its position and rotation to teleport it to the opposite window. By doing the same thing to the camera itself when it passes through windows and keeping careful track of the color changes, we get a perfect illusion of an infinite series of rooms with the rendering cost of just one.</p><p class="body-text">When the domain and codomain are the same, linear maps can be <em>iterated</em>: applied over and over to an input. If we take four specific maps from <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> to itself (let&#x2019;s call them <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> <span class="tex-holder inline-math" data-source-tex="B">$B$,</span> <span class="tex-holder inline-math" data-source-tex="C">$C$,</span> and <span class="tex-holder inline-math" data-source-tex="D">$D$),</span> we can repeatedly apply them to a starting point and see where it goes. At each step, we take a random map out of the four and apply it to a starting point of <span class="tex-holder inline-math" data-source-tex="(0, 0)">$(0, 0)$,</span> then take a random map and apply it to that point, and so on. We&#x2019;ll plot the points it visits &mdash; the brighter the point, the more frequently it&#x2019;s been there. </p><div class="desmos-border canvas-container"><canvas id="barnsley-fern-canvas" class="output-canvas"></canvas></div><p class="body-text">The result for these four maps in particular is called the <em>Barnsley fern</em>, named after its creator &mdash; amazingly, with enough iterations (10 million in this example), the picture converges to the same thing, even with the random choices of maps. Strictly speaking, these are <em>affine</em> linear maps, which means they multiply by a matrix and then add a vector, but it&#x2019;s a good demonstration nevertheless.</p><p class="body-text">As one final application, we can use linear maps to create systems of linear <em>differential</em> equations, not just algebraic ones. For example, the system</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]x' &= x - y\\[NEWLINE][TAB]y' &= x + y[NEWLINE]\end{align*}">$$\begin{align*}x' &= x - y\\[4px]y' &= x + y\end{align*}$$</span></p><p class="body-text">is really a matrix equation in disguise:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c} x' \\ y' \end{array}\right] = \left[\begin{array}{cc} 1& -1 \\ 1& 1 \end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c} x' \\ y' \end{array}\right] = \left[\begin{array}{cc} 1& -1 \\ 1& 1 \end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right].\end{align*}$$</span></p><p class="body-text">A great way to visualize these is by plotting a field of moving particles, where the velocity of one at <span class="tex-holder inline-math" data-source-tex="(x, y)">$(x, y)$</span> is given by this formula for <span class="tex-holder inline-math" data-source-tex="(x', y')">$(x', y')$.</span> Coloring the points by their velocity and direction, we get a striking picture.</p><div class="desmos-border canvas-container"><canvas id="vector-field-canvas" class="output-canvas"></canvas></div><p class="body-text">We&#x2019;ll continue to touch on topics from all of these examples throughout the term. If you&#x2019;d like to see more from any of them, they come from interactive applets I&#x2019;ve written &mdash; have a look!</p><div class="image-links"><div class="image-link"><a href="/applets/thurston-geometries/" tabindex="-1"><img src="/graphics/general-icons/placeholder.png" data-src="/applets/thurston-geometries/cover.webp" alt="Thurston Geometries" tabindex="1"></img></a><p class="image-link-subtext">Thurston Geometries</p></div><div class="image-link"><a href="/applets/barnsley-fern/" tabindex="-1"><img src="/graphics/general-icons/placeholder.png" data-src="/applets/barnsley-fern/cover.webp" alt="The Barnsley Fern" tabindex="1"></img></a><p class="image-link-subtext">The Barnsley Fern</p></div><div class="image-link"><a href="/applets/vector-fields/" tabindex="-1"><img src="/graphics/general-icons/placeholder.png" data-src="/applets/vector-fields/cover.webp" alt="Vector Fields" tabindex="1"></img></a><p class="image-link-subtext">Vector Fields</p></div></div></section><h2 class="section-text">Eigenvectors</h2><section><p class="body-text">Let&#x2019;s start with a topic from the second example: iterated linear maps. If we&#x2019;re trying to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}\vec{v}">$A^{100}\vec{v}$</span> for an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and a vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$,</span> then we&#x2019;d desperately like a trick to avoid doing <span class="tex-holder inline-math" data-source-tex="100">$100$</span> matrix-vector products. The best possible scenario would be if we had a basis <span class="tex-holder inline-math" data-source-tex="\{ \vec{v_1}, ..., \vec{v_n} \}">$\{ \vec{v_1}, ..., \vec{v_n} \}$</span> of fixed points for <span class="tex-holder inline-math" data-source-tex="A">$A$:</span> vectors so that <span class="tex-holder inline-math" data-source-tex="A\vec{v_i} = \vec{v_i}">$A\vec{v_i} = \vec{v_i}$.</span> Then for any vector <span class="tex-holder inline-math" data-source-tex="\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}">$\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}$,</span> we&#x2019;d have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]Av &= A(c_1 \vec{v_1} + \cdots + c_n \vec{v_n})\\[NEWLINE][TAB]&= c_1 A \vec{v_1} + \cdots + c_n A \vec{v_n}\\[NEWLINE][TAB]&= c_1 \vec{v_1} + \cdots + c_n \vec{v_n}\\[NEWLINE][TAB]&= \vec{v},[NEWLINE]\end{align*}">$$\begin{align*}Av &= A(c_1 \vec{v_1} + \cdots + c_n \vec{v_n})\\[4px]&= c_1 A \vec{v_1} + \cdots + c_n A \vec{v_n}\\[4px]&= c_1 \vec{v_1} + \cdots + c_n \vec{v_n}\\[4px]&= \vec{v},\end{align*}$$</span></p><p class="body-text">so <span class="tex-holder inline-math" data-source-tex="A^{100}\vec{v} = \vec{v}">$A^{100}\vec{v} = \vec{v}$,</span> and we&#x2019;ve sidestepped the problem entirely. Unfortunately, this calculation shows that having a basis of fixed points implies that <em>every</em> vector is a fixed point of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> meaning <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has to be the identity matrix <span class="tex-holder inline-math" data-source-tex="I">$I$.</span> The reality of what we can expect will turn out to not be that far off, though, as surprising as that may be. Let&#x2019;s take a look at an example to get our bearings. With <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$,</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> defined as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A &= \left[\begin{array}{cc} 5& 6 \\ -3& -4 \end{array}\right]\\[NEWLINE][TAB]\vec{v_1} &= \left[\begin{array}{c} -2 \\ 1 \end{array}\right]\\[NEWLINE][TAB]\vec{v_2} &= \left[\begin{array}{c} 1 \\ -1 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}A &= \left[\begin{array}{cc} 5& 6 \\ -3& -4 \end{array}\right]\\[4px]\vec{v_1} &= \left[\begin{array}{c} -2 \\ 1 \end{array}\right]\\[4px]\vec{v_2} &= \left[\begin{array}{c} 1 \\ -1 \end{array}\right],\end{align*}$$</span></p><p class="body-text">we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A\vec{v_1} &= \left[\begin{array}{c} -4 \\ 2 \end{array}\right] = 2\vec{v_1}\\[NEWLINE][TAB]A\vec{v_2} &= \left[\begin{array}{c} -1 \\ 1 \end{array}\right] = -\vec{v_2}.[NEWLINE]\end{align*}">$$\begin{align*}A\vec{v_1} &= \left[\begin{array}{c} -4 \\ 2 \end{array}\right] = 2\vec{v_1}\\[4px]A\vec{v_2} &= \left[\begin{array}{c} -1 \\ 1 \end{array}\right] = -\vec{v_2}.\end{align*}$$</span></p><p class="body-text">Neither of these is a fixed point, but they&#x2019;re the next best thing. If we want to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}\vec{v_1}">$A^{100}\vec{v_1}$,</span> it&#x2019;s just <span class="tex-holder inline-math" data-source-tex="2^{100}\vec{v_1}">$2^{100}\vec{v_1}$,</span> since every application of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> just multiplies by 2. Similarly, <span class="tex-holder inline-math" data-source-tex="A^{100}\vec{v_2} = (-1)^{100}\vec{v_2} = \vec{v_2}">$A^{100}\vec{v_2} = (-1)^{100}\vec{v_2} = \vec{v_2}$.</span> And if we want to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}\vec{v}">$A^{100}\vec{v}$</span> for any other vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> all we have to do is express <span class="tex-holder inline-math" data-source-tex="\vec{v} = c_1\vec{v_1} + c_2\vec{v_2}">$\vec{v} = c_1\vec{v_1} + c_2\vec{v_2}$</span> (since <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> are linearly independent and therefore form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$),</span> and then we can apply powers of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with no trouble at all. These objects are important enough that we&#x2019;ll want to give them a name:</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: eigenvectors and eigenvalues</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. An <strong>eigenvector</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is a nonzero vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$</span> such that <span class="tex-holder inline-math" data-source-tex="A\vec{v} = \lambda \vec{v}">$A\vec{v} = \lambda \vec{v}$</span> for some number <span class="tex-holder inline-math" data-source-tex="\lambda \in \mathbb{R}">$\lambda \in \mathbb{R}$,</span> called the <strong>eigenvalue</strong> corresponding to <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$.</span></p></div><p class="body-text"><em>Eigen</em> is German for <em>characteristic</em> (as well as <em>strange</em>), and refers to the way these objects are intrinsic to the matrix they come from. Eigenvectors and eigenvalues are defined only for square matrices, since if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t square, its outputs have different lengths than its inputs, so there&#x2019;s no hope of the two being multiples of one another. We also exclude the zero vector since <span class="tex-holder inline-math" data-source-tex="A\vec{0} = \vec{0}">$A\vec{0} = \vec{0}$</span> no matter what <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is, and the eigenvalue wouldn&#x2019;t even be defined.</p><p class="body-text">So &mdash; how can we find the eigenvectors and eigenvalues of a matrix from scratch? We can rewrite the equation <span class="tex-holder inline-math" data-source-tex="A\vec{v} = \lambda \vec{v}">$A\vec{v} = \lambda \vec{v}$</span> as <span class="tex-holder inline-math" data-source-tex="A\vec{v} - \lambda \vec{v} = \vec{0}">$A\vec{v} - \lambda \vec{v} = \vec{0}$,</span> and to express the left side as a single linear map acting on <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> we can write it as <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} = \vec{0}">$(A - \lambda I)\vec{v} = \vec{0}$.</span> If we knew <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> we could solve for <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> by row reduction, we usually won&#x2019;t know the eigenvalues ahead of time. To that end, let&#x2019;s start by finding all the possible values for <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$.</span> If <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} = \vec{0}">$(A - \lambda I)\vec{v} = \vec{0}$,</span> then <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \ker (A - \lambda I)">$\vec{v} \in \ker (A - \lambda I)$,</span> which means <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$</span> isn&#x2019;t one-to-one. An equivalent condition is that <span class="tex-holder inline-math" data-source-tex="\det(A - \lambda I) = 0">$\det(A - \lambda I) = 0$,</span> and that&#x2019;s an equation we can solve. Since the determinant is just a complicated series of multiplications and additions, the result will be a polynomial in <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> which we call the <strong>characteristic polynomial</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and denote <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$.</span> Once we solve <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda) = 0">$\chi_A(\lambda) = 0$</span> for the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> we can then plug each into <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} = \vec{0}">$(A - \lambda I)\vec{v} = \vec{0}$</span> in turn to find the corresponding eigenvectors.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: eigenvalues and eigenvectors</div><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">To get the characteristic polynomial, we subtract <span class="tex-holder inline-math" data-source-tex="\lambda I">$\lambda I$,</span> which just means subtracting <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> from every entry in the diagonal, and then take the determinant.</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det (A - \lambda I) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[NEWLINE][TAB]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[NEWLINE][TAB]&= -10 - 3\lambda + \lambda^2 + 6\\[NEWLINE][TAB]&= \lambda^2 - 3\lambda - 4\\[NEWLINE][TAB]&= (\lambda - 4)(\lambda + 1).[NEWLINE]\end{align*}">$$\begin{align*}\det (A - \lambda I) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[4px]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[4px]&= -10 - 3\lambda + \lambda^2 + 6\\[4px]&= \lambda^2 - 3\lambda - 4\\[4px]&= (\lambda - 4)(\lambda + 1).\end{align*}$$</span></p><p class="body-text">Since the characteristic polynomial is supposed to equal zero, the eigenvalues are <span class="tex-holder inline-math" data-source-tex="4">$4$</span> and <span class="tex-holder inline-math" data-source-tex="-1">$-1$.</span> We&#x2019;ll handle them one at a time to find the eigenvectors.</p><p class="body-text">First, let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="\lambda = 4">$\lambda = 4$.</span> That gives us the system <span class="tex-holder inline-math" data-source-tex="(A - 4 I)\vec{v} = 0">$(A - 4 I)\vec{v} = 0$,</span> which corresponds to the augmented matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= -2\vec{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= -2\vec{r_1}.\end{align*}$$</span></p><p class="body-text">In the form of an equation, <span class="tex-holder inline-math" data-source-tex="\vec{v}_1 - 3\vec{v}_2 = 0">$\vec{v}_1 - 3\vec{v}_2 = 0$,</span> so <span class="tex-holder inline-math" data-source-tex="\vec{v}_1 = 3\vec{v}_2">$\vec{v}_1 = 3\vec{v}_2$.</span> We just need one eigenvector, so let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="\vec{v}_2 = 1">$\vec{v}_2 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v}_1 = 3">$\vec{v}_1 = 3$</span> to get our first eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 3 \\ 1 \end{array}\right][NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 3 \\ 1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">with <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 4">$\lambda_1 = 4$.</span> For the other eigenvalue, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= -\frac{1}{3}\vec{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= -\frac{1}{3}\vec{r_1}.\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="6\vec{v}_1 - 3\vec{v}_2 = 0">$6\vec{v}_1 - 3\vec{v}_2 = 0$,</span> so <span class="tex-holder inline-math" data-source-tex="2\vec{v}_1 = \vec{v}_2">$2\vec{v}_1 = \vec{v}_2$.</span> We&#x2019;ll just take <span class="tex-holder inline-math" data-source-tex="\vec{v}_1 = 1">$\vec{v}_1 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v}_2 = 2">$\vec{v}_2 = 2$</span> to get our second eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: eigenvectors and eigenvalues</div><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].\end{align*}$$</span></p></div><p class="body-text">There&#x2019;s an important fact relating eigenvalues and determinants that we should take the opportunity to write down. Since the eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> of an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are just roots of its characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$,</span> we can write that polynomial as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\chi_A(\lambda) = (\lambda_1 - \lambda)(\lambda_2 - \lambda) \cdots (\lambda_n - \lambda).[NEWLINE]$$">$$\begin{align*}\chi_A(\lambda) = (\lambda_1 - \lambda)(\lambda_2 - \lambda) \cdots (\lambda_n - \lambda).\end{align*}$$</span></p><p class="body-text">On the other hand, <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda) = \det(A - \lambda I)">$\chi_A(\lambda) = \det(A - \lambda I)$.</span> If we set <span class="tex-holder inline-math" data-source-tex="\lambda = 0">$\lambda = 0$</span> on both sides, we find that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\lambda_1 \lambda_2 \cdots \lambda_n = \det A.[NEWLINE]$$">$$\begin{align*}\lambda_1 \lambda_2 \cdots \lambda_n = \det A.\end{align*}$$</span></p><p class="body-text">In other words,</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: eigenvalues and the determinant</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. Then <span class="tex-holder inline-math" data-source-tex="\det A">$\det A$</span> is the product of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p></div><p class="body-text">An immediate corollary is that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is invertible exactly if no eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is zero. This description makes the volume-scaling interpretation of the determinant quite a bit more clear: for example, the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} 4& -2 \\ 1& 1 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} 4& -2 \\ 1& 1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">has eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \end{array}\right]">$\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \end{array}\right]$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2} = \left[\begin{array}{c} 2 \\ 1 \end{array}\right]">$\vec{v_2} = \left[\begin{array}{c} 2 \\ 1 \end{array}\right]$,</span> with corresponding eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 3">$\lambda_1 = 3$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 2">$\lambda_2 = 2$.</span> Using these vectors and their images as sides of parallelograms shows exactly how <span class="tex-holder inline-math" data-source-tex="A">$A$</span> scales area: the purple and blue vectors are <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$,</span> and the red and green ones are <span class="tex-holder inline-math" data-source-tex="A\vec{v_1} = 3\vec{v_1}">$A\vec{v_1} = 3\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="A\vec{v_2} = 2\vec{v_2}">$A\vec{v_2} = 2\vec{v_2}$,</span> respectively.</p><div class="desmos-border"><div id="areaScaling" class="desmos-container"></div></div><p class="body-text">As is becoming increasingly clear, we&#x2019;d always like a <em>basis</em> of eigenvectors for a matrix. The characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span> of an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix is always of degree <span class="tex-holder inline-math" data-source-tex="n">$n$,</span> since there&#x2019;s exactly one term in the determinant expansion that involves multiplying together every diagonal entry of <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$,</span> each of which is of the form <span class="tex-holder inline-math" data-source-tex="a_{ii} - \lambda">$a_{ii} - \lambda$.</span> That means that we&#x2019;ll at least have <span class="tex-holder inline-math" data-source-tex="n">$n$</span> eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> but that number is counted with multiplicity &mdash; the eigenvalues could be repeated or even complex numbers. We&#x2019;ll dig more into those possibilities in future sections, but for now, we can at least verify that when all the eigenvalues are distinct, we get the basis of eigenvectors we&#x2019;re looking for.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: distinct eigenvalues guarantee a basis of eigenvectors</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix with eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> so that <span class="tex-holder inline-math" data-source-tex="\lambda_i \neq \lambda_j">$\lambda_i \neq \lambda_j$</span> for <span class="tex-holder inline-math" data-source-tex="i \neq j">$i \neq j$.</span> Then the corresponding eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$</span> form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">We&#x2019;ll often get the chance to prove our results in this course, and it&#x2019;s worth taking it whenever we can. While this course isn&#x2019;t proof-based, it&#x2019;s good preparation for future ones that are, and there&#x2019;s usually value in seeing <em>why</em> results are true regardless.</p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$</span> is a list of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> we only need to check if they&#x2019;re linearly independent or they span &mdash; if one is true, the other must be (this might be a more familiar fact if the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> are placed as columns in a matrix; then the two conditions are that the corresponding map is one-to-one or onto). Linear independence is usually an easier condition to check, so let&#x2019;s try that.</p><p class="body-text">We&#x2019;ll show this result one vector at a time, first showing that <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> are linearly independent. If <span class="tex-holder inline-math" data-source-tex="c_1\vec{v_1} + c_2\vec{v_2} = \vec{0}">$c_1\vec{v_1} + c_2\vec{v_2} = \vec{0}$,</span> then all we can really do is apply <span class="tex-holder inline-math" data-source-tex="A">$A$</span> to both sides &mdash; otherwise, the <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> don&#x2019;t appear. The result of doing that is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1 \lambda_1 \vec{v_1} + c_2 \lambda_2 \vec{v_2} = \vec{0},[NEWLINE]$$">$$\begin{align*}c_1 \lambda_1 \vec{v_1} + c_2 \lambda_2 \vec{v_2} = \vec{0},\end{align*}$$</span></p><p class="body-text">which looks very similar to our original equation. Multiplying it by <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1 \lambda_1 \vec{v_1} + c_2 \lambda_1 \vec{v_2} = \vec{0},[NEWLINE]$$">$$\begin{align*}c_1 \lambda_1 \vec{v_1} + c_2 \lambda_1 \vec{v_2} = \vec{0},\end{align*}$$</span></p><p class="body-text">and subtracting these two equations from one another, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]c_2 (\lambda_2 - \lambda_1) \vec{v_2} &= \vec{0}\\[NEWLINE][TAB]c_2 (\lambda_2 - \lambda_1) \vec{v_2} &= \vec{0}.[NEWLINE]\end{align*}">$$\begin{align*}c_2 (\lambda_2 - \lambda_1) \vec{v_2} &= \vec{0}\\[4px]c_2 (\lambda_2 - \lambda_1) \vec{v_2} &= \vec{0}.\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> is an eigenvector, it&#x2019;s not the zero vector, and <span class="tex-holder inline-math" data-source-tex="\lambda_1 \neq \lambda_2">$\lambda_1 \neq \lambda_2$</span> by assumption, so <span class="tex-holder inline-math" data-source-tex="c_2 = 0">$c_2 = 0$.</span> Then <span class="tex-holder inline-math" data-source-tex="c_1 = 0">$c_1 = 0$</span> too since <span class="tex-holder inline-math" data-source-tex="\vec{v_1} \neq \vec{0}">$\vec{v_1} \neq \vec{0}$,</span> and so <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> are linearly independent.</p><p class="body-text">This process continues to work: if we know <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k}">$\vec{v_1}, ..., \vec{v_k}$</span> are linearly independent, then we can show <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_{k + 1}}">$\vec{v_1}, ..., \vec{v_{k + 1}}$</span> are. That must mean that <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$</span> are linearly independent &mdash; intuitively, it&#x2019;s because we can repeat the process until we reach all <span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors, but if you&#x2019;ve taken a proofs class, you&#x2019;ll recognize this as an example of mathematical induction. Regardless, <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span> is a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span></p></div><p class="body-text">One way to quickly put these ideas to use is with <em>dynamical systems</em>, a fancy term that just means a system whose state changes over time. When that change is characterized by a linear map, we can analyze its long-term behavior most easily by finding a basis of eigenvectors.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: a dynamical system</div><p class="body-text">A particular university has exclusively math, physics, and computer science majors, and doesn&#x2019;t allow double majors. Every year, 25% of the math majors at the university switch to a computer science major, 50% of the physics majors switch to computer science, and 25% of the computer science majors switch to physics. If there are initially 100 students in each of the three majors and no one ever graduates, how many will be in each major in the long run?</p><p class="body-text">The three states here are being a math, physics, and computer science major &mdash; if <span class="tex-holder inline-math" data-source-tex="\vec{x}(t) \in \mathbb{R}^3">$\vec{x}(t) \in \mathbb{R}^3$</span> gives the number of students in each major after <span class="tex-holder inline-math" data-source-tex="t">$t$</span> years, then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{x}(0) = \left[\begin{array}{c} 100 \\ 100 \\ 100 \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{x}(0) = \left[\begin{array}{c} 100 \\ 100 \\ 100 \end{array}\right],\end{align*}$$</span></p><p class="body-text">and for any nonnegative integer <span class="tex-holder inline-math" data-source-tex="t">$t$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{x}(t + 1) = A\vec{x}(t)">$\vec{x}(t + 1) = A\vec{x}(t)$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 0.75& 0& 0 \\ 0& 0.5& 0.25 \\ 0.25& 0.5& 0.75 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 0.75& 0& 0 \\ 0& 0.5& 0.25 \\ 0.25& 0.5& 0.75 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The entry in row <span class="tex-holder inline-math" data-source-tex="i">$i$</span> and column <span class="tex-holder inline-math" data-source-tex="j">$j$</span> is the proportion of people in state <span class="tex-holder inline-math" data-source-tex="j">$j$</span> that move to state <span class="tex-holder inline-math" data-source-tex="i">$i$</span> in each year &mdash; since each column contains proportions of every state, their entries all sum to 1.</p><p class="body-text">The long-term behavior of the system is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\lim_{t \to \infty} \vec{x}(t) = \lim_{t \to \infty} A^t \vec{x}(0),[NEWLINE]$$">$$\begin{align*}\lim_{t \to \infty} \vec{x}(t) = \lim_{t \to \infty} A^t \vec{x}(0),\end{align*}$$</span></p><p class="body-text">so we want to express <span class="tex-holder inline-math" data-source-tex="\vec{x}(0)">$\vec{x}(0)$</span> in a basis of eigenvectors. To find those eigenvectors, we first find <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det \left[\begin{array}{ccc} 0.75 - \lambda& 0& 0 \\ 0& 0.5 - \lambda& 0.25 \\ 0.25& 0.5& 0.75 - \lambda \end{array}\right] &= 0\\[NEWLINE][TAB]-\lambda^3 + 2\lambda^2 - 1.1875\lambda + 0.1875 &= 0[NEWLINE]\end{align*}">$$\begin{align*}\det \left[\begin{array}{ccc} 0.75 - \lambda& 0& 0 \\ 0& 0.5 - \lambda& 0.25 \\ 0.25& 0.5& 0.75 - \lambda \end{array}\right] &= 0\\[4px]-\lambda^3 + 2\lambda^2 - 1.1875\lambda + 0.1875 &= 0\end{align*}$$</span></p><p class="body-text">Without the ability to solve cubics easily, we might as well plug this into an equation solver. The resulting solutions are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\lambda_1 = 1, \quad \lambda_2 = 0.75, \quad \lambda_3 = 0.25.[NEWLINE]$$">$$\begin{align*}\lambda_1 = 1, \quad \lambda_2 = 0.75, \quad \lambda_3 = 0.25.\end{align*}$$</span></p><p class="body-text">These are all distinct, and so our previous theorem guarantees that we&#x2019;ll have a basis of eigenvectors. To start, let&#x2019;s solve for <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0.75 - 1& 0& 0 & 0 \\ 0& 0.5 - 1& 0.25 & 0 \\ 0.25& 0.5& 0.75 - 1 & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} -0.25& 0& 0 & 0 \\ 0& -0.5& 0.25 & 0 \\ 0.25& 0.5& -0.25 & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 1& 2& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -4 \\ \vec{r_2} \ \times\!\!= -2 \\ \vec{r_3} \ \times\!\!= 4 \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 2& -1 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 0.75 - 1& 0& 0 & 0 \\ 0& 0.5 - 1& 0.25 & 0 \\ 0.25& 0.5& 0.75 - 1 & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} -0.25& 0& 0 & 0 \\ 0& -0.5& 0.25 & 0 \\ 0.25& 0.5& -0.25 & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 1& 2& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -4 \\ \vec{r_2} \ \times\!\!= -2 \\ \vec{r_3} \ \times\!\!= 4 \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 2& -1 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\end{align*}$$</span></p><p class="body-text">In total,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 0 \\ t/2 \\ t \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 0 \\ t/2 \\ t \end{array}\right],\end{align*}$$</span></p><p class="body-text">so with <span class="tex-holder inline-math" data-source-tex="t = 2">$t = 2$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Repeating this process for the next two vectors,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} -2 \\ 1 \\ 1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 0 \\ -1 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_2} = \left[\begin{array}{c} -2 \\ 1 \\ 1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 0 \\ -1 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Now we need to express <span class="tex-holder inline-math" data-source-tex="\vec{x}(0)">$\vec{x}(0)$</span> as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{x}(0) = c_1 \vec{v_1} + c_2 \vec{v_2} + c_3 \vec{v_3},[NEWLINE]$$">$$\begin{align*}\vec{x}(0) = c_1 \vec{v_1} + c_2 \vec{v_2} + c_3 \vec{v_3},\end{align*}$$</span></p><p class="body-text">which we can accomplish by inverting the matrix with the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> as columns:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc} 0& -2& 0 \\ 1& 1& -1 \\ 2& 1& 1 \end{array}\right]^{-1} &= \left[\begin{array}{ccc} 1/3& 1/3& 1/3 \\ -1/2& 0& 0 \\ -1/6& -2/3& 1/3 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{ccc} 1/3& 1/3& 1/3 \\ -1/2& 0& 0 \\ -1/6& -2/3& 1/3 \end{array}\right]\left[\begin{array}{c} 100 \\ 100 \\ 100 \end{array}\right] = \left[\begin{array}{c} 100 \\ -50 \\ -50 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc} 0& -2& 0 \\ 1& 1& -1 \\ 2& 1& 1 \end{array}\right]^{-1} &= \left[\begin{array}{ccc} 1/3& 1/3& 1/3 \\ -1/2& 0& 0 \\ -1/6& -2/3& 1/3 \end{array}\right]\\[4px]\left[\begin{array}{ccc} 1/3& 1/3& 1/3 \\ -1/2& 0& 0 \\ -1/6& -2/3& 1/3 \end{array}\right]\left[\begin{array}{c} 100 \\ 100 \\ 100 \end{array}\right] = \left[\begin{array}{c} 100 \\ -50 \\ -50 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Then </p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\lim_{t \to \infty} A^t \vec{x}(0) &= \lim_{t \to \infty} A^t (100 \vec{v_1} + -50 \vec{v_2} + -50 \vec{v_3})\\[NEWLINE][TAB]&= \lim_{t \to \infty} (100 \cdot 1^t \vec{v_1} -50 \cdot 0.75^t \vec{v_2} - 50 \cdot 0.25^t \vec{v_3})\\[NEWLINE][TAB]&= 100 \vec{v_1}\\[NEWLINE][TAB]&= \left[\begin{array}{c} 0 \\ 100 \\ 200 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\lim_{t \to \infty} A^t \vec{x}(0) &= \lim_{t \to \infty} A^t (100 \vec{v_1} + -50 \vec{v_2} + -50 \vec{v_3})\\[4px]&= \lim_{t \to \infty} (100 \cdot 1^t \vec{v_1} -50 \cdot 0.75^t \vec{v_2} - 50 \cdot 0.25^t \vec{v_3})\\[4px]&= 100 \vec{v_1}\\[4px]&= \left[\begin{array}{c} 0 \\ 100 \\ 200 \end{array}\right].\end{align*}$$</span></p><p class="body-text">In the long run, there will be 100 students in the physics major and 200 in the computer science major (and tragically, no one in math).</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: a dynamical system</div><p class="body-text">Solutions A and B are mixed in a chemical reaction. Every minute, 25% of the mass of solution A is converted to solution B, and 40% of the mass of solution B is converted back to A. If there are initially 10 grams of solution A and 25 of solution B, how much will there be in the long run?</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>