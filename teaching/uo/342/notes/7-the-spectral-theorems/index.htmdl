### nav-buttons

In the first part of the course, we studied bases of eigenvectors, and in the second part, orthonormal bases. These two notions are somewhat in conflict: while we can always construct an orthonormal basis for a vector space with the Gram-Schmidt process, applying that to a basis of eigenvectors for a diagonalizable matrix generally won't preserve the fact that they're eigenvectors. What we'll explore in this last part of the course is the situation in which we can get the best of both worlds --- an orthonormal basis of eigenvectors. Without any more preliminaries, let's state and prove the theorem that characterizes exactly when this is possible.

### thm "The Real Spectral Theorem"

	A matrix $A$ with real entries has an orthonormal basis of eigenvectors with real eigenvalues if and only if $A$ is symmetric (i.e. $A^T = A$).

###

We'll see the proof of this shortly; to keep it from being overwhelmingly long, we'll split it into a few chunks that we can tackle one at a time.

### lem "real symmetric matrices have real eigenvalues"
	
	Let $A$ be an $n \times n$ symmetric matrix with real entries. Then all of the eigenvalues of $A$ are real.

###

### pf
	
	Let $\vec{v} \in #C#^n$ be an eigenvector with eigenvalue $\lambda \in #C#$ --- we have to assume both are complex since we haven't yet shown that they have to be real. Multiplying $A$ on the right by $\vec{v}$ and the left by $\overline{\vec{v}}^T$, we have

	$$
		\overline{\vec{v}^T} A \vec{v} &= \overline{\vec{v}^T} \lambda \vec{v}

		&= \lambda \left( \vec{v} \bullet \overline{\vec{v}} \right).
	$$

	Now $\vec{v} \bullet \overline{\vec{v}}$ is a real number, since

	$$
		\overline{\vec{v} \bullet \overline{\vec{v}}} &= \overline{\vec{v}} \bullet \vec{v}

		&= \vec{v} \bullet \overline{\vec{v}},
	$$

	and the same is true for $\overline{\vec{v}^T} A \vec{v}$:

	$$
		\overline{\overline{\vec{v}^T} A \vec{v}} &= \vec{v}^T \overline{A} \overline{\vec{v}}

		&= \vec{v}^T A \overline{\vec{v}}

		&= \vec{v}^T A^T \overline{\vec{v}}

		&= \left( \overline{\vec{v}}^T A \vec{v} \right)^T

		&= \overline{\vec{v}}^T A \vec{v}.
	$$

	All that is to say that $\lambda$ itself must also be real, since

	$$

		\lambda = \frac{\overline{\vec{v}^T} A \vec{v}}{\vec{v} \bullet \overline{\vec{v}}}.
	$$

###

This lemma also implies that all the eigenvectors of a real symmetric matrix are real, since row reducing $A - \lambda I$ for a real-valued matrix $A$ and a real eigenvalue $\lambda$ will only produce real solutions.

### lem "real symmetric matrices have orthogonal eigenspaces"

	Let $A$ be an $n \times n$ symmetric matrix with real entries and let $\vec{v}$ and $\vec{w}$ be eigenvectors corresponding to distinct eigenvalues $\lambda_i$ and $\lambda_j$. Then $\vec{v} \bullet \vec{w} = 0$.

###

### pf

	We can verify this with a direct computation:

	$$
		(\lambda_i - \lambda_j) \left( \vec{v} \bullet \vec{w} \right) &= \lambda_i \left( \vec{v} \bullet \vec{w} \right) - \lambda_j \left( \vec{v} \bullet \vec{w} \right)

		&= \left( \lambda_i \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( \lambda_j \vec{w} \right)

		&= \left( A \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( A \vec{w} \right)

		&= \left( A \vec{v} \right)^T \vec{w} - \vec{v}^T \left( A \vec{w} \right)

		&= \vec{v}^T A^T \vec{w} - \vec{v}^T A \vec{w}

		&= \vec{v}^T A \vec{w} - \vec{v}^T A \vec{w}

		&= 0.
	$$

	Since $\lambda_i \neq \lambda_j$, $\lambda_i - \lambda_j \neq 0$, and so $\vec{v} \bullet \vec{w} = 0$.

###

Our final lemma is considerably more technical, but it'll let us put all the pieces together.

### lem "symmetric matrices have symmetric restrictions"

	Let $A$ be an $n \times n$ symmetric matrix, let $\vec{v}$ be an eigenvector of $A$ and eigenvalue $\lambda$, and let $X = \span\left\{ \vec{v} \right\}$ Then if $\vec{w} \in X^\perp$, $A\vec{w} \in X^\perp$ too, and if $A|_{X^\perp} : X^\perp \to X^\perp$ is the linear map given by restricting $A$ to $X^\perp$, then there is a basis for $X^\perp$ in which the matrix for $A|_{X^\perp}$ is symmetric.

###

### pf

	There are a ton of symbols here, but the moral of this lemma is that $A$ can be split cleanly into its action on two subspaces: $X = \span\left\{ \vec{v} \right\}$ and $X^\perp$. We already know that $A : X \to X$, since $A \vec{v} = \lambda \vec{v} \in X$, and now we'll also show that $A : X^\perp \to X^\perp$ as the lemma states.

	Let $\vec{w} \in X^\perp$. Then $\vec{w} \bullet \vec{v} = 0$, and so

	$$
		\left( A\vec{w} \right) \bullet \vec{v} &= \left( A\vec{w} \right)^T \bullet \vec{v}

		&= \vec{w}^T A^T \vec{v}

		&= \vec{w}^T A \vec{v}

		&= \vec{w}^T \lambda \vec{v}

		&= \lambda \left( \vec{w} \bullet \vec{v} \right)

		&= 0,
	$$

	meaning $A\vec{w} \in X^\perp$.

	Now let $\left\{ \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$ be an orthonormal basis for $X^\perp$ (via the Gram-Schmidt process). We can freely assume $\left| \left| \vec{v} \right| \right| = 1$ since it's an eigenvector, and so what we've shown is that the matrix of $A$ with respect to the orthonormal basis $\left\{ \vec{v}, \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$ for $#R#^n$ is

	$$
		[[ \lambda | 0, \cdots, 0 ; \hline 0 | ,, ; \vdots | , C, ; 0 | ,, ]]
	$$

	for some matrix $C$ --- the second statement of this lemma is that $C$ is also symmetric. If

	$$
		B = [[ \mid, \mid, , \mid ; \vec{v}, \vec{w_1}, \cdots, \vec{w_{n - 1}} ; \mid, \mid, , \mid ]],
	$$

	then what we've shown is that

	$$
		B^{-1}AB = [[ \lambda | 0, \cdots, 0 ; \hline 0 | ,, ; \vdots | , C, ; 0 | ,, ]],
	$$

	and so transposing both sides results in

	$$
		B^T A \left( B^{-1} \right)^T = [[ \lambda | 0, \cdots, 0 ; \hline 0 | ,, ; \vdots | , C^T, ; 0 | ,, ]].
	$$

	But since $B$ is a matrix whose columns form an orthonormal basis, it's unitary, and so $B^T = B^{-1}$. Therefore,

	$$
		B^{-1} A B = [[ \lambda | 0, \cdots, 0 ; \hline 0 | ,, ; \vdots | , C^T, ; 0 | ,, ]],
	$$

	and so $C = C^T$, proving the result.

###

At long last, we're ready to return to the Spectral theorem itself.

### pf -m "Proof of the Real Spectral Theorem"

	To begin, let's show that if $A$ has an orthonormal basis $\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$ of eigenvectors, then it must be symmetric. This is just a direct computation: we know $A = BDB^{-1}$, but $B$ must be a unitary matrix, and so $B^{-1} = B^T$. Then

	$$
		A^T &= \left( BDB^T \right)^T

		&= \left( B^T \right)^T D^T B^T

		&= BDB^T

		&= A,
	$$

	so $A$ is symmetric.

	The other direction is where all the complexity lies. If $A$ is a symmetric matrix with real entries, then by the first lemma, all of its eigenvalues and eigenvectors are real. By the second, any two eigenvectors from different eigenspaces are orthogonal, and we can arrange for eigenvectors within a single eigenspace to be orthogonal too: if $E_i$ is the eigenspace corresponding to $\lambda_i$, then we can just run the Gram-Schmidt process on $E_i$ to produce an orthogonal basis.

	We're almost done! We've shown that all the eigenvalues and eigenvectors are real, and that the eigenvectors are also orthogonal. The only substantive thing left to show is that there are enough eigenvectors --- that the algebraic multiplicity of every eigenvalue is the same as the geometric multiplicity. This is where the final lemma comes in. Every matrix is guaranteed to have one eigenvector at the very least (the worst-case scenario is that there's only a single eigenvalue with algebraic multiplicity $n$ and geometric multiplicity $1$). Let's call that guaranteed eigenvector for $A$ $\vec{v_1}$. The third lemma tells us that if we take an orthonormal basis for $\left( \span\left\{ \vec{v_1} \right\} \right)^\perp$, then $A$ is still a symmetric matrix when expressed in that basis. That means it's guaranteed to have another real eigenvector $\vec{v_2}$ that's orthogonal to $\vec{v_1}$, and so we can repeat this process $n$ times until all that's left is a $1 \times 1$ matrix, which is always diagonalizable (since it's diagonal). In total, we have $n$ distinct, orthogonal eigenvectors with real eigenvectors, and to cap off the proof, we can rescale every eigenvector to have magnitude $1$, making them orthonormal.

###

That was a ton of work! It's easily the most complicated theorem we'll prove in the course. To wrap up this section, we'll state the more general version for complex matrices for completeness, although we won't have many uses for it in this class.

### thm "The Complex Spectral Theorem"

	Let $A$ be an $n \times n$ matrix, and denote by $A^*$ the **conjugate transpose** of $A$; i.e. $A^* = \overline{A}^T$. Then $A$ has an orthonormal basis of eigenvectors if and only if $A^* A = A A^*$ (which is called being a **normal** operator).

###

We're approaching the end of the course! So far, we've discussed eigenvectors and eigenvalues and inner products, and this section has concerned where the two intersect --- specifically, that symmetric matrices are the ones that are diagonalizable with an orthonormal basis of eigenvalues. In the remaining two sections, we'll develop theory for the less-ideal cases: first, a way to very nearly diagonalize square nondiagonalizable matrices, and then a diagonalization-like process that applies to any matrix, square or not, and has wide-ranging applications.

### nav-buttons