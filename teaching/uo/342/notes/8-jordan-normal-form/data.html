<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 8: Jordan Normal Form</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">The matrix equation <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> isn&#x2019;t always solvable, but when there&#x2019;s no solution, we can still use the least-squares methods of section 5 to find a best approximation. In this section, we&#x2019;ll take a similar approach to diagonalization: if a square matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t diagonalizable, it&#x2019;s because some eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> has algebraic multiplicity of <span class="tex-holder inline-math" data-source-tex="k > 1">$k > 1$,</span> but fewer than <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent eigenvectors with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$.</span> For example,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} -3& 9 \\ -4& 9 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} -3& 9 \\ -4& 9 \end{array}\right]\end{align*}$$</span></p><p class="body-text">has characteristic polynomial <span class="tex-holder inline-math" data-source-tex="(\lambda - 3)^2">$(\lambda - 3)^2$,</span> so the only eigenvalue is <span class="tex-holder inline-math" data-source-tex="\lambda = 3">$\lambda = 3$.</span> Solving for the eigenvectors, we only have one:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right],\end{align*}$$</span></p><p class="body-text">meaning <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is nondiagonalizable. There may not be a basis of eigenvectors, but let&#x2019;s see if we can still find some basis that&#x2019;s helpful. We&#x2019;d definitely like to use <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> as one of the two vectors in this prospective basis, and since there&#x2019;s no linearly independent vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> with <span class="tex-holder inline-math" data-source-tex="(A - 3I)\vec{v} = \vec{0}">$(A - 3I)\vec{v} = \vec{0}$,</span> the next-best thing to hope for is that there&#x2019;s one so that <span class="tex-holder inline-math" data-source-tex="(A - 3I)\vec{v} = \vec{v_1}">$(A - 3I)\vec{v} = \vec{v_1}$.</span> Checking that, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} -6& 9 & 3 \\ -4& 6 & 2 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& -3 & -1 \\ 2& -3 & -1 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -\frac{1}{2} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& 3 & -1 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= 2\vec{r_1}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} -6& 9 & 3 \\ -4& 6 & 2 \end{array}\right] &\\[4px]\left[\begin{array}{cc|c} 2& -3 & -1 \\ 2& -3 & -1 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -\frac{1}{2} \end{array}\\[4px]\left[\begin{array}{cc|c} 2& 3 & -1 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= 2\vec{r_1}\end{align*}$$</span></p><p class="body-text">In total, we have a solution of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left[\begin{array}{c} -1 - 3t \\ 2t \end{array}\right],[NEWLINE]$$">$$\begin{align*}\left[\begin{array}{c} -1 - 3t \\ 2t \end{array}\right],\end{align*}$$</span></p><p class="body-text">so with <span class="tex-holder inline-math" data-source-tex="t = 0">$t = 0$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} -1 \\ 0 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_2} = \left[\begin{array}{c} -1 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">With the basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, \vec{v_2} \right\}">$\left\{ \vec{v_1}, \vec{v_2} \right\}$,</span> the matrix representation of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t so bad! Specifically, <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{cc} 3& -1 \\ 2& 0 \end{array}\right] \qquad D = \left[\begin{array}{cc} 3& 1 \\ 0& 3 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{cc} 3& -1 \\ 2& 0 \end{array}\right] \qquad D = \left[\begin{array}{cc} 3& 1 \\ 0& 3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t diagonalizable, something like this is as close as we&#x2019;re going to get. In fact, this is remarkably close to a diagonalization, with the sole exception of the <span class="tex-holder inline-math" data-source-tex="1">$1$</span> off the diagonal of <span class="tex-holder inline-math" data-source-tex="D">$D$</span> that means the eigenspace corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda = 3">$\lambda = 3$</span> has dimension <span class="tex-holder inline-math" data-source-tex="1">$1$</span> instead of <span class="tex-holder inline-math" data-source-tex="2">$2$.</span> With the goal of bringing this almost-diagonalization to any square matrix, let&#x2019;s back up and state a few definitions that formalize what we&#x2019;ve seen here.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: generalized eigenvector</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix and <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> an eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> A <strong>generalized eigenvector</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> is a vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> so that <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)^k\vec{v} = \vec{0}">$(A - \lambda I)^k\vec{v} = \vec{0}$</span> for some <span class="tex-holder inline-math" data-source-tex="k \geq 1">$k \geq 1$.</span> The <strong>rank</strong> of a generalized eigenvector is the smallest <span class="tex-holder inline-math" data-source-tex="k">$k$</span> for which this holds. </p></div><p class="body-text">Let&#x2019;s connect this back to what we&#x2019;ve seen. Rank 1 generalized eigenvectors are just regular eigenvectors, and for rank 2 ones, we have <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)^k\vec{v} = \vec{0}">$(A - \lambda I)^k\vec{v} = \vec{0}$</span> for <span class="tex-holder inline-math" data-source-tex="k = 2">$k = 2$</span> but not <span class="tex-holder inline-math" data-source-tex="k = 1">$k = 1$</span> &mdash; or in other words,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB](A - \lambda I)\vec{v} \in \ker (A - \lambda I)[NEWLINE]$$">$$\begin{align*}(A - \lambda I)\vec{v} \in \ker (A - \lambda I)\end{align*}$$</span></p><p class="body-text">but <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} \neq \vec{0}">$(A - \lambda I)\vec{v} \neq \vec{0}$,</span> which is exactly what it means for <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v}">$(A - \lambda I)\vec{v}$</span> to be an eigenvector of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$.</span> Similarly, the rank 3 generalized eigenvectors are those which are sent to rank 2 ones when plugged into <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$.</span></p><p class="body-text">We&#x2019;ll omit the proof of the next result to avoid getting too bogged down in the weeds, but it will let us find bases for any matrix which are as close to diagonal as possible.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: basis of generalized eigenvectors</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix and let <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> be an eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with algebraic multiplicity <span class="tex-holder inline-math" data-source-tex="k">$k$.</span> Then there are exactly <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent generalized eigenvectors corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> and so there is a basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> of generalized eigenvectors of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p></div><p class="body-text">When expressed in such a basis, the matrix for <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has all its eigenvalues down the diagonal, but there are some <span class="tex-holder inline-math" data-source-tex="1">$1$s</span> on the diagonal above the middle one (called the <strong>superdiagonal</strong>) for strings of generalized eigenvectors, and this form is said to be in <strong>Jordan normal form</strong>.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: Jordan normal form</div><p class="body-text">Express the matrix <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{ccc} -1& -1& -2 \\ 1& 1& 1 \\ 4& 0& 5 \end{array}\right]">$A = \left[\begin{array}{ccc} -1& -1& -2 \\ 1& 1& 1 \\ 4& 0& 5 \end{array}\right]$</span> in Jordan normal form.</p><p class="body-text">First, the characteristic polynomial is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= \lambda^3 + 5\lambda^2 - 8\lambda + 4\\[NEWLINE][TAB]&= (\lambda - 1)(\lambda - 2)^2.[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= \lambda^3 + 5\lambda^2 - 8\lambda + 4\\[4px]&= (\lambda - 1)(\lambda - 2)^2.\end{align*}$$</span></p><p class="body-text">The eigenvector for <span class="tex-holder inline-math" data-source-tex="\lambda = 1">$\lambda = 1$</span> is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -2& -1& -2 & 0 \\ 1& 0& 1 & 0 \\ 4& 0& 4 & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -1& 0 & 0 \\ 1& 0& 1 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 2\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} -1 \\ 0 \\ 1 \end{array}\right]&,[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -2& -1& -2 & 0 \\ 1& 0& 1 & 0 \\ 4& 0& 4 & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 0& -1& 0 & 0 \\ 1& 0& 1 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 2\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[4px]\vec{v_1} = \left[\begin{array}{c} -1 \\ 0 \\ 1 \end{array}\right]&,\end{align*}$$</span></p><p class="body-text">and for <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -1& -2 & 0 \\ 1& -1& 1 & 0 \\ 4& 0& 3 & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 0 \\ 0& 4& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 3\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& -4& 4 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= 4\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& 0& 3 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} -3 \\ 1 \\ 4 \end{array}\right] &.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -3& -1& -2 & 0 \\ 1& -1& 1 & 0 \\ 4& 0& 3 & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 0 \\ 0& 4& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 3\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& -4& 4 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= 4\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& 0& 3 & 0\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[4px]\vec{v_2} = \left[\begin{array}{c} -3 \\ 1 \\ 4 \end{array}\right] &.\end{align*}$$</span></p><p class="body-text">There isn&#x2019;t a basis of eigenvectors, but we know there should be a second generalized eigenvector for <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$,</span> since its algebraic multiplicity is <span class="tex-holder inline-math" data-source-tex="2">$2$</span> but its geometric multiplicity is <span class="tex-holder inline-math" data-source-tex="1">$1$.</span> To find it, we solve <span class="tex-holder inline-math" data-source-tex="A - 2I = \vec{v_2}">$A - 2I = \vec{v_2}$,</span> but the left side of the row reduction is identical, since it&#x2019;s the same matrix:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -1& -2 & -3 \\ 1& -1& 1 & 1 \\ 4& 0& 3 & 4 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 1 \\ 0& 4& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 3\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & -1 \\ 1& -1& 1 & 1\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& -4& 4 & 4\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= 4\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& 0& 3 & 4\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\vec{v_3} = \left[\begin{array}{c} 1 - 3t \\ t \\ 4t \end{array}\right]&.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -3& -1& -2 & -3 \\ 1& -1& 1 & 1 \\ 4& 0& 3 & 4 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 1& -1& 1 & 1 \\ 0& 4& -1 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 3\vec{r_2} \\ \vec{r_3} \ -\!\!= 4\vec{r_2} \end{array}\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & -1 \\ 1& -1& 1 & 1\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& -4& 4 & 4\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= 4\\[4px]\left[\begin{array}{ccc|c} 0& -4& 1 & 0 \\ 4& 0& 3 & 4\\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[4px]\vec{v_3} = \left[\begin{array}{c} 1 - 3t \\ t \\ 4t \end{array}\right]&.\end{align*}$$</span></p><p class="body-text">Here we&#x2019;re taking care to spell out the free parameter fully. When we&#x2019;re solving for eigenvectors normally, we set <span class="tex-holder inline-math" data-source-tex="t">$t$</span> to a convenient nonzero value, since the zero vector is the only disallowed one. Here, though, <span class="tex-holder inline-math" data-source-tex="t = 0">$t = 0$</span> is a perfectly fine choice, and in fact the simplest one, since it doesn&#x2019;t produce the zero vector. Therefore, we&#x2019;ll take</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_3} = \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{v_3} = \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right],\end{align*}$$</span></p><p class="body-text">and so we&#x2019;ve expressed our matrix as <span class="tex-holder inline-math" data-source-tex="A = BJB^{-1}">$A = BJB^{-1}$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{ccc} \mid& \mid& \mid \\ \vec{v_1}& \vec{v_2}& \vec{v_3} \\ \mid& \mid& \mid \end{array}\right] \qquad J = \left[\begin{array}{ccc} 1& 0& 0 \\ 0& 2& 1 \\ 0& 0& 2 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{ccc} \mid& \mid& \mid \\ \vec{v_1}& \vec{v_2}& \vec{v_3} \\ \mid& \mid& \mid \end{array}\right] \qquad J = \left[\begin{array}{ccc} 1& 0& 0 \\ 0& 2& 1 \\ 0& 0& 2 \end{array}\right].\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: Jordan normal form</div><p class="body-text">Express the matrix <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{ccc} -2& 10& -1 \\ -4& 11& -1 \\ -9& 18& 0 \end{array}\right]">$A = \left[\begin{array}{ccc} -2& 10& -1 \\ -4& 11& -1 \\ -9& 18& 0 \end{array}\right]$</span> in Jordan normal form.</p></div><p class="body-text">When an eigenvalue has more than one eigenvector corresponding to it, the situation can be somewhat more complex: for example, a <span class="tex-holder inline-math" data-source-tex="3 \times 3">$3 \times 3$</span> matrix with a single eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> but only two eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> does have a generalized eigenvector, but it might not be a solution to either <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} = \vec{v_1}">$(A - \lambda I)\vec{v} = \vec{v_1}$</span> or <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} = \vec{v_2}">$(A - \lambda I)\vec{v} = \vec{v_2}$.</span> Instead, it might only be a solution to a specific linear combination <span class="tex-holder inline-math" data-source-tex="c_1\vec{v_1} + c_2\vec{v_2}">$c_1\vec{v_1} + c_2\vec{v_2}$.</span> We won&#x2019;t have problems where we solve for generalized eigenvectors in cases like these, but it&#x2019;s worth being aware that they exist.</p><p class="body-text">We&#x2019;re almost done! Out last section will deal with another diagonalization-like process, but one where the matrix need not even be square.</p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>