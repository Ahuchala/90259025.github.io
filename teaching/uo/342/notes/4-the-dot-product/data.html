<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 4: The Dot Product</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">So far in linear algebra, we&#x2019;ve focused on vectors as fairly abstract concepts. Vectors can look like lists of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> numbers in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> polynomials in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}[x]">$\mathbb{R}[x]$,</span> or even matrices or linear transformations. But largely speaking, we&#x2019;ve refrained from talking about the <em>geometry</em> of these spaces. Vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> typically represented as arrows in the plane, have notions of length and direction in addition to the component representation we&#x2019;re more familiar with, and there&#x2019;s often a lot to be gained from thinking about vectors in this way. There&#x2019;s also the <strong>dot product</strong>, which takes two vectors and produces a number that measures their lengths and the angle between them, and in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$</span> specifically, there&#x2019;s also the <strong>cross product</strong>, which takes two vectors and produces a third vector that&#x2019;s orthogonal (i.e. perpendicular) to them.</p><p class="body-text">So what can we expect to generalize to arbitrary vector spaces? Probably not the cross product, since it&#x2019;s not even defined in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> unless <span class="tex-holder inline-math" data-source-tex="n = 3">$n = 3$</span> (there are technically generalizations to higher dimensions, but they&#x2019;re still specific to <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> and won&#x2019;t generalize past that). Also, the notion of a vector having a single direction in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> breaks down immediately in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$,</span> where we need to know two angles and a length (i.e. spherical coordinates) to pin down a vector. In an <span class="tex-holder inline-math" data-source-tex="n">$n$-dimensional</span> space, there are <span class="tex-holder inline-math" data-source-tex="n">$n$</span> degrees of freedom, so we&#x2019;d need <span class="tex-holder inline-math" data-source-tex="n - 1">$n - 1$</span> angles to keep track of a vector if we know its length, and that&#x2019;s probably not a realistic goal. On the other hand, the angle <em>between</em> two vectors works just fine in every <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> as does the dot product. In fact, everything that feels like it should generalize &mdash; the angle between vectors, orthogonality, and magnitude &mdash; can be defined in terms of the dot product. We&#x2019;ll see how it works in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> in this section, apply it to some common applications in the next, and then we&#x2019;ll bring its ideas to more general vector spaces in the section after that.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: the dot product</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span> The <strong>dot product</strong> of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$,</span> written <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w}">$\vec{v} \bullet \vec{w}$,</span> is defined by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right] \bullet \left[\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.[NEWLINE]$$">$$\begin{align*}\left[\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right] \bullet \left[\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.\end{align*}$$</span></p></div><p class="body-text">It&#x2019;s not particularly clear why this is something we&#x2019;d care about initially, especially when <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w}">$\vec{v} \bullet \vec{w}$</span> isn&#x2019;t even a vector in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> To start, let&#x2019;s write down a few fundamental properties of the dot product, and then we&#x2019;ll look at what it can be used for.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: properties of the dot product</div><p class="body-text numbered-list-item">1. The dot product is <strong>symmetric</strong>: <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}">$\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span></p><p class="body-text numbered-list-item">2. The dot product is <strong>bilinear</strong>:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}[NEWLINE]$$">$$\begin{align*}\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}\end{align*}$$</span></p><p class="body-text">and</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)[NEWLINE]$$">$$\begin{align*}\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)\end{align*}$$</span></p><p class="body-text">for all <span class="tex-holder inline-math" data-source-tex="\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$</span> and <span class="tex-holder inline-math" data-source-tex="c \in \mathbb{R}">$c \in \mathbb{R}$.</span></p><p class="body-text numbered-list-item">3. The dot product is <strong>positive definite</strong>: <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{v} \geq 0">$\vec{v} \bullet \vec{v} \geq 0$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$,</span> and the only <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> for which <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{v} = 0">$\vec{v} \bullet \vec{v} = 0$</span> is <span class="tex-holder inline-math" data-source-tex="\vec{v} = 0">$\vec{v} = 0$.</span></p></div><p class="body-text">The last property lets us define a notion of length in terms of only the dot product, finally connecting us to some of the geometry that we haven&#x2019;t been dealing with directly.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: magnitude</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$.</span> The <strong>magnitude</strong> of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> also called the <strong>length</strong> or <strong>norm</strong>, is <span class="tex-holder inline-math" data-source-tex="||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}">$||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}$.</span></p></div><p class="body-text">This is exactly the notion of length we&#x2019;re used to: the vector <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} 2 \\ -3 \end{array}\right] \in \mathbb{R}^2">$\vec{v} = \left[\begin{array}{c} 2 \\ -3 \end{array}\right] \in \mathbb{R}^2$</span> has magnitude</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}\\[NEWLINE][TAB]&= \sqrt{(2)(2) + (-3)(-3)}\\[NEWLINE][TAB]&= \sqrt{13}.[NEWLINE]\end{align*}">$$\begin{align*}||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}\\[4px]&= \sqrt{(2)(2) + (-3)(-3)}\\[4px]&= \sqrt{13}.\end{align*}$$</span></p><p class="body-text">By dividing a nonzero vector by its magnitude, we produce a parallel vector with magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$.</span> This is called <strong>normalizing</strong>, and any vector with magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$</span> is called a <strong>unit vector</strong>. For example, the vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> from before normalizes to the unit vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\frac{\vec{v}}{||\vec{v}||} = \left[\begin{array}{c} \frac{2}{\sqrt{13}} \\ -\frac{3}{\sqrt{13}} \end{array}\right].[NEWLINE]$$">$$\begin{align*}\frac{\vec{v}}{||\vec{v}||} = \left[\begin{array}{c} \frac{2}{\sqrt{13}} \\ -\frac{3}{\sqrt{13}} \end{array}\right].\end{align*}$$</span></p><p class="body-text">The magnitude of a vector is implicitly its distance from the zero vector, and that notion generalizes to the distance <em>between</em> two vectors as well.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: distance between vectors</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span> The <strong>distance</strong> between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> is <span class="tex-holder inline-math" data-source-tex="||\vec{v} - \vec{w}||">$||\vec{v} - \vec{w}||$.</span></p></div><p class="body-text">Again, this is exactly the usual distance between points in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span></p><div class="desmos-border"><div id="vectorSubtraction" class="desmos-container"></div></div><p class="body-text">Here, the purple vector is <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right]">$\vec{v} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right]$,</span> the blue one is <span class="tex-holder inline-math" data-source-tex="\vec{w} = \left[\begin{array}{c} -2 \\ 1 \end{array}\right]">$\vec{w} = \left[\begin{array}{c} -2 \\ 1 \end{array}\right]$,</span> and the distance between them is the magnitude of the red vector, which is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} - \vec{w} = \left[\begin{array}{c} 3 - (-2) \\ 2 - 1 \end{array}\right] = \left[\begin{array}{c} 5 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v} - \vec{w} = \left[\begin{array}{c} 3 - (-2) \\ 2 - 1 \end{array}\right] = \left[\begin{array}{c} 5 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Let&#x2019;s use these vectors to demonstrate one more property of the dot product: its ability to measure the angle between vectors. We&#x2019;ll need the rarely-used <strong>law of cosines</strong>, which states that for any triangle with sides of length <span class="tex-holder inline-math" data-source-tex="a">$a$,</span> <span class="tex-holder inline-math" data-source-tex="b">$b$,</span> and <span class="tex-holder inline-math" data-source-tex="c">$c$,</span> and angle <span class="tex-holder inline-math" data-source-tex="\theta">$\theta$</span> opposite <span class="tex-holder inline-math" data-source-tex="c">$c$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c^2 = a^2 + b^2 - 2ab\cos(\theta).[NEWLINE]$$">$$\begin{align*}c^2 = a^2 + b^2 - 2ab\cos(\theta).\end{align*}$$</span></p><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="\theta">$\theta$</span> is the angle between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$,</span> then we can see from the previous graph and the law of cosines that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).[NEWLINE]\end{align*}">$$\begin{align*}||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).\end{align*}$$</span></p><p class="body-text">We generally need to be careful when applying inverse trig functions, but everything&#x2019;s fine here since <span class="tex-holder inline-math" data-source-tex="\arccos">$\arccos$</span> has a range of <span class="tex-holder inline-math" data-source-tex="[0, \pi]">$[0, \pi]$,</span> which is exactly the interval of possible angles between vectors.</p><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: distance and angle between vectors</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> be the vectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right], \quad \vec{w} = \left[\begin{array}{c} 0 \\ -3 \\ 3 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right], \quad \vec{w} = \left[\begin{array}{c} 0 \\ -3 \\ 3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Find the distance between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> and the angle between them.</p></div><p class="body-text">This definition of angle lets us talk about perpendicular vectors. If the angle between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> is exactly <span class="tex-holder inline-math" data-source-tex="90^\circ">$90^\circ$,</span> then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.[NEWLINE]$$">$$\begin{align*}\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.\end{align*}$$</span></p><p class="body-text">Since we&#x2019;re defining everything in terms of the dot product, let&#x2019;s do that here too.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: orthogonality</div><p class="body-text">Two vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$</span> are <strong>orthogonal</strong> if <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><p class="body-text">We can immediately start getting applications of orthogonality: notably, the Pythagorean theorem can be stated in the language of linear algebra.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">The Pythagorean Theorem</div><p class="body-text">Two vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$</span> are <strong>orthogonal</strong> if and only if <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: the Pythagorean theorem</div><p class="body-text">Show that this theorem holds. Start by assuming <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> are orthogonal and show <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.</span> Then assume that <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$</span> and show that <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> must be orthogonal.</p></div><p class="body-text">Orthogonality will be surprisingly critical to most of the rest of the class. We&#x2019;ll want to talk often about all the vectors orthogonal to a given one, and so we&#x2019;ll give that object a name.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: orthogonal complement</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> The <strong>orthogonal complement</strong> to <span class="tex-holder inline-math" data-source-tex="X">$X$</span> is the subspace <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> (pronounced &#x201C;X perp&#x201D;, like perpendicular) consisting of the vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> that are orthogonal to every vector in <span class="tex-holder inline-math" data-source-tex="X">$X$.</span></p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: orthogonal complement</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] \right\}">$X = \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] \right\}$.</span> Find a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$.</span></p><p class="body-text">For a vector <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right]">$\vec{v} = \left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right]$</span> to be in the orthogonal complement, we need</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right] &= 0\\[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] &= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right] &= 0\\[4px]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] &= 0,\end{align*}$$</span></p><p class="body-text">so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]v_1 + v_2 &= 0\\[NEWLINE][TAB]-v_2 + 2v_3 &= 0.[NEWLINE]\end{align*}">$$\begin{align*}v_1 + v_2 &= 0\\[4px]-v_2 + 2v_3 &= 0.\end{align*}$$</span></p><p class="body-text">And that&#x2019;s a matrix equation once again! Row reducing, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 1& 0 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \quad \vec{r_1} \ +\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& 1& -2 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -1\\[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} -2t \\ 2t \\ t \end{array}\right] & .[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 1& 1& 0 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \quad \vec{r_1} \ +\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& 1& -2 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -1\\[4px]\vec{v} = \left[\begin{array}{c} -2t \\ 2t \\ t \end{array}\right] & .\end{align*}$$</span></p><p class="body-text">In total, the orthogonal complement is <span class="tex-holder inline-math" data-source-tex="X^\perp = \operatorname{span}\left\{ \left[\begin{array}{c} -2 \\ 2 \\ 1 \end{array}\right] \right\}">$X^\perp = \operatorname{span}\left\{ \left[\begin{array}{c} -2 \\ 2 \\ 1 \end{array}\right] \right\}$.</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: orthogonal complement</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> Show that <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> is in fact a subspace.</p></div></section><h2 class="section-text">Orthonormal Bases</h2><section><p class="body-text">When we&#x2019;re asked to come up with a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> there are two generally good choices we&#x2019;ve seen at this point: if we have some diagonalizable <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> that&#x2019;s relevant to the situation, we might want to choose a basis of eigenvectors of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> and otherwise, the standard basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{e_1}, ..., \vec{e_n} \right\}">$\left\{ \vec{e_1}, ..., \vec{e_n} \right\}$</span> is a reliable choice. Let&#x2019;s dig into that a little more and understand <em>why</em> it&#x2019;s so reliable. The standard basis has many useful properties, but let&#x2019;s focus on the most relevant:</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: orthonormal</div><p class="body-text">A collection of vectors <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> is <strong>orthogonal</strong> if <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \bullet \vec{v_j} = 0">$\vec{v_i} \bullet \vec{v_j} = 0$</span> whenever <span class="tex-holder inline-math" data-source-tex="i \neq j">$i \neq j$,</span> and it is <strong>orthonormal</strong> if additionally, <span class="tex-holder inline-math" data-source-tex="||\vec{v_i}|| = 1">$||\vec{v_i}|| = 1$</span> for every <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span></p></div><p class="body-text">When a basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> is <strong>orthonormal</strong>, finding the expression of a vector in terms of the basis is much, <em>much</em> easier than it otherwise would be. Normally, that process requires inverting a change-of-basis matrix, but now we can sidestep that entirely. Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$</span> be a vector with the expression</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}.[NEWLINE]$$">$$\begin{align*}\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}.\end{align*}$$</span></p><p class="body-text">Now take the dot product of both sides with <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{v} \bullet \vec{v_i} &= \left( c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right) \bullet \vec{v_i}\\[NEWLINE][TAB]&= c_1 \vec{v_1} \bullet \vec{v_i} + \cdots + c_n \vec{v_n} \bullet \vec{v_i}\\[NEWLINE][TAB]&= c_i.[NEWLINE]\end{align*}">$$\begin{align*}\vec{v} \bullet \vec{v_i} &= \left( c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right) \bullet \vec{v_i}\\[4px]&= c_1 \vec{v_1} \bullet \vec{v_i} + \cdots + c_n \vec{v_n} \bullet \vec{v_i}\\[4px]&= c_i.\end{align*}$$</span></p><p class="body-text">In other words,</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: expression in an orthonormal basis</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> be an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> Then for any <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left( \vec{v} \bullet \vec{v_1} \right) \vec{v_1} + \cdots + \left( \vec{v} \bullet \vec{v_n} \right) \vec{v_n}.[NEWLINE]$$">$$\begin{align*}\vec{v} = \left( \vec{v} \bullet \vec{v_1} \right) \vec{v_1} + \cdots + \left( \vec{v} \bullet \vec{v_n} \right) \vec{v_n}.\end{align*}$$</span></p></div><p class="body-text">Another point of convenience is that any collection of nonzero orthonormal vectors &mdash; or even just orthogonal ones &mdash; is necessarily linearly independent, so a collection of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of them in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> must be a basis.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: orthogonal vectors are linearly independent</div><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n">$\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n$</span> are orthogonal and nonzero, then they are linearly independent.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">Suppose <span class="tex-holder inline-math" data-source-tex="c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}">$c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}$.</span> Dotting both sides by <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_i \left( \vec{v_i} \bullet \vec{v_i} \right) \vec{v_i} = 0,[NEWLINE]$$">$$\begin{align*}c_i \left( \vec{v_i} \bullet \vec{v_i} \right) \vec{v_i} = 0,\end{align*}$$</span></p><p class="body-text">and since <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \neq 0">$\vec{v_i} \neq 0$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \bullet \vec{v_i} \neq 0">$\vec{v_i} \bullet \vec{v_i} \neq 0$,</span> meaning <span class="tex-holder inline-math" data-source-tex="c_i = 0">$c_i = 0$.</span> In total, every <span class="tex-holder inline-math" data-source-tex="c_i = 0">$c_i = 0$,</span> and so the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> are linearly independent.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: expression in an orthonormal basis</div><p class="body-text">Given the vectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 1 \\ -2 \\ 1 \end{array}\right], \quad \vec{v_2} = \left[\begin{array}{c} -1 \\ -1 \\ -1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 3 \\ 0 \\ -3 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 1 \\ -2 \\ 1 \end{array}\right], \quad \vec{v_2} = \left[\begin{array}{c} -1 \\ -1 \\ -1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 3 \\ 0 \\ -3 \end{array}\right].\end{align*}$$</span></p><p class="body-text numbered-list-item">1. Show that <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, \vec{v_2}, \vec{v_3} \right\}">$\left\{ \vec{v_1}, \vec{v_2}, \vec{v_3} \right\}$</span> is an orthogonal set, and therefore a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$.</span></p><p class="body-text numbered-list-item">2. Find an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}">$\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$,</span> where <span class="tex-holder inline-math" data-source-tex="\vec{w_i}">$\vec{w_i}$</span> is parallel to <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span></p><p class="body-text numbered-list-item">3. Express the vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 4 \\ 3 \\ -1 \end{array}\right][NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 4 \\ 3 \\ -1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">in the basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}">$\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$.</span></p></div><p class="body-text">Just like linear independence is closely related to invertibility of matrices &mdash; in that <span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> are linearly independent if and only if the matrix with them as columns is invertible &mdash; orthonormality is closely related to another property a matrix can have.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: unitary matrix</div><p class="body-text">An <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix is <strong>unitary</strong> if its columns are orthonormal.</p></div><p class="body-text">Unitary matrices don&#x2019;t have to be square, but any square unitary matrix has to be invertible, since the columns have to be linearly independent. Even non-square unitary matrices have a sort of pseudoinverse in their transpose: if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is unitary, then <span class="tex-holder inline-math" data-source-tex="A^T\!A = I">$A^T\!A = I$,</span> since every entry in a matrix product is really just a dot product in disguise, and the rows of <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> are the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> by definition. Putting these two facts together, a square unitary matrix is always invertible, and <span class="tex-holder inline-math" data-source-tex="A^{-1} = A^T">$A^{-1} = A^T$.</span></p><p class="body-text">We&#x2019;ll have more to say about unitary matrices later on, but for now, let&#x2019;s see a few nice properties they have.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: properties of unitary matrices</div><p class="body-text"> Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be a unitary <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix. Then <span class="tex-holder inline-math" data-source-tex="A">$A$</span> preserves distances and angles: <span class="tex-holder inline-math" data-source-tex="\left| \left| A\vec{v} \right| \right| = \left| \left| \vec{v} \right| \right|">$\left| \left| A\vec{v} \right| \right| = \left| \left| \vec{v} \right| \right|$</span> and <span class="tex-holder inline-math" data-source-tex="\left( A\vec{v} \right) \bullet \left( A\vec{w} \right) = \vec{v} \bullet \vec{w}">$\left( A\vec{v} \right) \bullet \left( A\vec{w} \right) = \vec{v} \bullet \vec{w}$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">We&#x2019;ll show that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> preserves distances. Let</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} c_1 \\ \vdots \\ c_n \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} c_1 \\ \vdots \\ c_n \end{array}\right],\end{align*}$$</span></p><p class="body-text">and denote the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> by <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$.</span> Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left| \left| A\vec{v} \right| \right|^2 &= \left| \left| c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right| \right|^2\\[NEWLINE][TAB]&= \left| c_1 \right|^2 \left| \left| \vec{v_1} \right| \right|^2 + \cdots + \left| c_n \right|^2 \left| \left| \vec{v_n} \right| \right|^2\\[NEWLINE][TAB]&= \left| c_1 \right|^2 + \cdots + \left| c_n \right|^2\\[NEWLINE][TAB]&= \left| \left| \vec{v} \right| \right|^2.[NEWLINE]\end{align*}">$$\begin{align*}\left| \left| A\vec{v} \right| \right|^2 &= \left| \left| c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right| \right|^2\\[4px]&= \left| c_1 \right|^2 \left| \left| \vec{v_1} \right| \right|^2 + \cdots + \left| c_n \right|^2 \left| \left| \vec{v_n} \right| \right|^2\\[4px]&= \left| c_1 \right|^2 + \cdots + \left| c_n \right|^2\\[4px]&= \left| \left| \vec{v} \right| \right|^2.\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: properties of unitary matrices</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be a unitary <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix. Show that <span class="tex-holder inline-math" data-source-tex="\left( A\vec{v} \right) \bullet \left( A\vec{w} \right) = \vec{v} \bullet \vec{w}">$\left( A\vec{v} \right) \bullet \left( A\vec{w} \right) = \vec{v} \bullet \vec{w}$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span></p></div></section><h2 class="section-text">The Gram-Schmidt Process</h2><section><p class="body-text">Now that we&#x2019;ve seen some of the nice properties of orthonormal bases, the natural next question is when we can find them. The good news is that unlike bases of eigenvectors of a matrix, which only exist when the matrix is diagonalizable, orthonormal bases always exist! We&#x2019;ll take a look at the standard process of finding one from a general basis, but in practice, we&#x2019;ll mainly use it to guarantee that an orthonormal basis <em>exists</em> rather than actually finding it.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">The Gram-Schmidt Process</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_1}, ..., \vec{x_n} \right\}">$\left\{ \vec{x_1}, ..., \vec{x_n} \right\}$</span> be a basis for a subspace <span class="tex-holder inline-math" data-source-tex="X">$X$</span> of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$.</span> Then an orthogonal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$</span> is <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{y_1}, ..., \vec{y_n} \right\}">$\left\{ \vec{y_1}, ..., \vec{y_n} \right\}$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{y_1} &= \vec{x_1}\\[NEWLINE][TAB]\vec{y_2} &= \vec{x_2} - \frac{\vec{x_2} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1}\\[NEWLINE][TAB]\vec{y_3} &= \vec{x_3} - \frac{\vec{x_3} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_3} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2}\\[NEWLINE][TAB]&\ \ \vdots[NEWLINE]\end{align*}">$$\begin{align*}\vec{y_1} &= \vec{x_1}\\[4px]\vec{y_2} &= \vec{x_2} - \frac{\vec{x_2} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1}\\[4px]\vec{y_3} &= \vec{x_3} - \frac{\vec{x_3} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_3} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2}\\[4px]&\ \ \vdots\end{align*}$$</span></p><p class="body-text">By normalizing every <span class="tex-holder inline-math" data-source-tex="\vec{y_i}">$\vec{y_i}$,</span> we can also produce an orthonormal basis.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">As with many results about orthogonality, this amounts to a long-winded computation, and we&#x2019;ll show it one vector at a time. To show <span class="tex-holder inline-math" data-source-tex="\vec{y_1}">$\vec{y_1}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{y_2}">$\vec{y_2}$</span> are orthogonal, we can just compute</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{y_1} \bullet \vec{y_2} &= \vec{x_1} \bullet \left( \vec{x_2} - \frac{\vec{x_2} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} \right)\\[NEWLINE][TAB]&= \vec{x_1} \bullet \vec{x_2} - \vec{x_1} \bullet \frac{\vec{x_2} \bullet \vec{x_1}}{\vec{x_1} \bullet \vec{x_1}}\vec{x_1}\\[NEWLINE][TAB]&= \vec{x_1} \bullet \vec{x_2} - \vec{x_2} \bullet \vec{x_1}\\[NEWLINE][TAB]&= 0.[NEWLINE]\end{align*}">$$\begin{align*}\vec{y_1} \bullet \vec{y_2} &= \vec{x_1} \bullet \left( \vec{x_2} - \frac{\vec{x_2} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} \right)\\[4px]&= \vec{x_1} \bullet \vec{x_2} - \vec{x_1} \bullet \frac{\vec{x_2} \bullet \vec{x_1}}{\vec{x_1} \bullet \vec{x_1}}\vec{x_1}\\[4px]&= \vec{x_1} \bullet \vec{x_2} - \vec{x_2} \bullet \vec{x_1}\\[4px]&= 0.\end{align*}$$</span></p><p class="body-text">For the general case, suppose we&#x2019;ve shown that <span class="tex-holder inline-math" data-source-tex="\vec{y_1}, ..., \vec{y_{k - 1}}">$\vec{y_1}, ..., \vec{y_{k - 1}}$</span> are all orthogonal. We&#x2019;ll show that <span class="tex-holder inline-math" data-source-tex="\vec{y_k}">$\vec{y_k}$</span> is orthogonal to all of them &mdash; if <span class="tex-holder inline-math" data-source-tex="1 \leq i \leq k - 1">$1 \leq i \leq k - 1$,</span> then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{y_k} \bullet \vec{y_i} &= \left( \vec{x_k} - \frac{\vec{x_k} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_k} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} - \cdots - \frac{\vec{x_k} \bullet \vec{y_{k - 1}}}{\vec{y_{k - 1}} \bullet \vec{y_{k - 1}}}\vec{y_{k - 1}} \right) \bullet \vec{y_i}\\[NEWLINE][TAB]&= \vec{x_k} \bullet \vec{y_i} - \frac{\vec{x_k} \bullet \vec{y_i}}{\vec{y_i} \bullet \vec{y_i}}\vec{y_i} \bullet \vec{y_i}\\[NEWLINE][TAB]&= 0.[NEWLINE]\end{align*}">$$\begin{align*}\vec{y_k} \bullet \vec{y_i} &= \left( \vec{x_k} - \frac{\vec{x_k} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_k} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} - \cdots - \frac{\vec{x_k} \bullet \vec{y_{k - 1}}}{\vec{y_{k - 1}} \bullet \vec{y_{k - 1}}}\vec{y_{k - 1}} \right) \bullet \vec{y_i}\\[4px]&= \vec{x_k} \bullet \vec{y_i} - \frac{\vec{x_k} \bullet \vec{y_i}}{\vec{y_i} \bullet \vec{y_i}}\vec{y_i} \bullet \vec{y_i}\\[4px]&= 0.\end{align*}$$</span></p><p class="body-text">As in past sections, you might recognize this as a proof by induction or just as a convincing argument, but either way, it shows that all the <span class="tex-holder inline-math" data-source-tex="\vec{y_i}">$\vec{y_i}$</span> are mutually orthogonal. That means they&#x2019;re linearly independent, and they&#x2019;re all still in <span class="tex-holder inline-math" data-source-tex="X">$X$</span> since they&#x2019;re just linear combinations of the <span class="tex-holder inline-math" data-source-tex="\vec{x_i}">$\vec{x_i}$,</span> and so they must form a basis.</p></div><p class="body-text">Intuitively, every step of the Gram-Schmidt process begins with <span class="tex-holder inline-math" data-source-tex="\vec{x_i}">$\vec{x_i}$</span> and removes any component of the previous vectors, leaving behind a vector that&#x2019;s orthogonal to every one that came before.</p><div class="desmos-border"><div id="gramSchmidt" class="desmos-container"></div></div><p class="body-text">In this Desmos graph, the purple and blue vectors are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{x_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right], \quad \vec{x_2} = \left[\begin{array}{c} -3 \\ 1 \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{x_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right], \quad \vec{x_2} = \left[\begin{array}{c} -3 \\ 1 \end{array}\right],\end{align*}$$</span></p><p class="body-text">respectively. Then <span class="tex-holder inline-math" data-source-tex="\vec{x_1} \bullet \vec{x_2} = -7">$\vec{x_1} \bullet \vec{x_2} = -7$,</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x_1} \bullet \vec{x_1} = 13">$\vec{x_1} \bullet \vec{x_1} = 13$.</span> The vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\frac{\vec{x_2} \bullet \vec{x_1}}{\vec{x_1} \bullet \vec{x_1}}\vec{x_1} &= -\frac{7}{13}\left[\begin{array}{c} 3 \\ 2 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\frac{\vec{x_2} \bullet \vec{x_1}}{\vec{x_1} \bullet \vec{x_1}}\vec{x_1} &= -\frac{7}{13}\left[\begin{array}{c} 3 \\ 2 \end{array}\right],\end{align*}$$</span></p><p class="body-text">which is what the Gram-Schmidt process will subtract from <span class="tex-holder inline-math" data-source-tex="\vec{x_2}">$\vec{x_2}$,</span> is shown in red. The final vector <span class="tex-holder inline-math" data-source-tex="\vec{y_2}">$\vec{y_2}$</span> is in green, drawn both as the geometric subtraction between the blue and red vectors, and again from the origin to highlight how it&#x2019;s orthogonal to the purple vector <span class="tex-holder inline-math" data-source-tex="\vec{x_1} = \vec{y_1}">$\vec{x_1} = \vec{y_1}$.</span> Notice how the red vector looks like the shadow cast on the line <span class="tex-holder inline-math" data-source-tex="\operatorname{span}\left\{ \vec{x_1} \right\}">$\operatorname{span}\left\{ \vec{x_1} \right\}$</span> by a faraway light source orthogonal to it &mdash; we&#x2019;ll have more to say about that later in this section.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: the Gram-Schmidt process</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> given by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] \right\}.[NEWLINE]$$">$$\begin{align*}X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] \right\}.\end{align*}$$</span></p><p class="body-text">Find an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$.</span></p><p class="body-text">Let&#x2019;s call those three basis vectors <span class="tex-holder inline-math" data-source-tex="\vec{x_1}">$\vec{x_1}$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{x_2}">$\vec{x_2}$,</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x_3}">$\vec{x_3}$.</span> We&#x2019;ll start by finding an orthogonal basis with the Gram-Schmidt process, beginning with</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{y_1} = \vec{x_1} = \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{y_1} = \vec{x_1} = \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="\vec{y_1} \bullet \vec{y_1} = 6">$\vec{y_1} \bullet \vec{y_1} = 6$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x_2} \bullet \vec{y_1} = 8">$\vec{x_2} \bullet \vec{y_1} = 8$,</span> and so our next vector is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right] - \frac{8}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] &= \left[\begin{array}{c} 2/3 \\ 1/3 \\ 1 \\ 4/3 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right] - \frac{8}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] &= \left[\begin{array}{c} 2/3 \\ 1/3 \\ 1 \\ 4/3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">We&#x2019;d like to avoid dealing with fractions if at all possible. Since this is orthogonal to <span class="tex-holder inline-math" data-source-tex="\vec{y_1}">$\vec{y_1}$,</span> rescaling it won&#x2019;t change that (and we&#x2019;ll have to do it anyway later when we normalize everything), so we might as well take</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{y_2} = \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{y_2} = \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Now our final vector is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{x_3} - \frac{\vec{x_3} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_3} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} &= \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] - \frac{-5}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] - \frac{14}{30}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c} -1/10 \\ 1/5 \\ -2/5 \\ 3/10 \end{array}\right]\\[NEWLINE][TAB]\vec{y_3} &= \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\vec{x_3} - \frac{\vec{x_3} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_3} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} &= \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] - \frac{-5}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] - \frac{14}{30}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right]\\[4px]&= \left[\begin{array}{c} -1/10 \\ 1/5 \\ -2/5 \\ 3/10 \end{array}\right]\\[4px]\vec{y_3} &= \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Sure enough, all these vectors are orthogonal. To make them orthonormal, we just need to normalize them, producing a final basis of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left\{ \frac{1}{\sqrt{6}}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] \right\}[NEWLINE]$$">$$\begin{align*}\left\{ \frac{1}{\sqrt{6}}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] \right\}\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: the Gram-Schmidt process</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> given by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right], \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ 3 \end{array}\right] \right\}.[NEWLINE]$$">$$\begin{align*}X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right], \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ 3 \end{array}\right] \right\}.\end{align*}$$</span></p><p class="body-text">Find an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$.</span></p></div></section><h2 class="section-text">Projections</h2><section><p class="body-text">There&#x2019;s another perspective on orthogonal bases that&#x2019;s worth discussing. If we express <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^3">$\vec{v} \in \mathbb{R}^3$</span> in the standard basis as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = c_1 \vec{e_1} + c_2 \vec{e_2} + c_3 \vec{e_3},[NEWLINE]$$">$$\begin{align*}\vec{v} = c_1 \vec{e_1} + c_2 \vec{e_2} + c_3 \vec{e_3},\end{align*}$$</span></p><p class="body-text">we can think about the vector <span class="tex-holder inline-math" data-source-tex="c_1 \vec{e_1}">$c_1 \vec{e_1}$</span> as being the vector in <span class="tex-holder inline-math" data-source-tex="\operatorname{span}\left\{ \vec{v_1} \right\}">$\operatorname{span}\left\{ \vec{v_1} \right\}$</span> that&#x2019;s closest to <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$.</span> Similarly, <span class="tex-holder inline-math" data-source-tex="c_1 \vec{e_1} + c_3 \vec{e_3}">$c_1 \vec{e_1} + c_3 \vec{e_3}$</span> is the vector in <span class="tex-holder inline-math" data-source-tex="\operatorname{span}\left\{ \vec{e_1}, \vec{e_3} \right\}">$\operatorname{span}\left\{ \vec{e_1}, \vec{e_3} \right\}$</span> closest to <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> where <em>closest</em> means minimizing <span class="tex-holder inline-math" data-source-tex="\left| \left| \vec{v} - \vec{w} \right| \right|">$\left| \left| \vec{v} - \vec{w} \right| \right|$</span> across all <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> in a subspace. <a href="https://www.desmos.com/3d/d1f12d74a5">This 3D Desmos graph</a> shows the red vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 2 \\ 1 \\ 3 \end{array}\right] \in \mathbb{R}^3[NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 2 \\ 1 \\ 3 \end{array}\right] \in \mathbb{R}^3\end{align*}$$</span></p><p class="body-text">and the closest vector in <span class="tex-holder inline-math" data-source-tex="\operatorname{span}\left\{ \vec{e_1}, \vec{e_2} \right\}">$\operatorname{span}\left\{ \vec{e_1}, \vec{e_2} \right\}$,</span> colored blue. Toggling on the surface <span class="tex-holder inline-math" data-source-tex="d(x, y)">$d(x, y)$</span> shows a 3D plot of the distances to the red vector, where the height of the surface at <span class="tex-holder inline-math" data-source-tex="(x, y)">$(x, y)$</span> is the distance to <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> from the vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{w} = \left[\begin{array}{c} x \\ y \\ 0 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{w} = \left[\begin{array}{c} x \\ y \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Unsurprisingly, the minimum is at <span class="tex-holder inline-math" data-source-tex="x = 2">$x = 2$</span> and <span class="tex-holder inline-math" data-source-tex="y = 1">$y = 1$.</span> We&#x2019;ll talk more about these closest approximations and how to find them in the next section, but let&#x2019;s focus on their existence and uniqueness right now.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: orthogonal decomposition</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> and <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> its orthogonal complement, and let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$.</span> Then <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> can be written uniquely as <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{x} + \vec{x}'">$\vec{v} = \vec{x} + \vec{x}'$</span> for <span class="tex-holder inline-math" data-source-tex="\vec{x} \in X">$\vec{x} \in X$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x}' \in X^\perp">$\vec{x}' \in X^\perp$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">This is a great example of how useful orthonormal bases can be &mdash; not only in computations, but also in proofs. Let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_1}, ..., \vec{x_k} \right\}">$\left\{ \vec{x_1}, ..., \vec{x_k} \right\}$</span> be an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$</span> (which exists via the Gram-Schmidt process), and extend it to a basis</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left\{ \vec{x_1}, ..., \vec{x_k}, \vec{x_{k + 1}}, ..., \vec{x_n} \right\}[NEWLINE]$$">$$\begin{align*}\left\{ \vec{x_1}, ..., \vec{x_k}, \vec{x_{k + 1}}, ..., \vec{x_n} \right\}\end{align*}$$</span></p><p class="body-text">for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> By applying the Gram-Schmidt process to this new basis, we can assume it too is orthonormal (and therefore <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_{k + 1}}, ..., \vec{x_n} \right\}">$\left\{ \vec{x_{k + 1}}, ..., \vec{x_n} \right\}$</span> is a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$).</span> Now <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> has a unique expression</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = c_1\vec{x_1} + \cdots + c_n \vec{x_n},[NEWLINE]$$">$$\begin{align*}\vec{v} = c_1\vec{x_1} + \cdots + c_n \vec{x_n},\end{align*}$$</span></p><p class="body-text">and so we can (uniquely) define</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{x} &= c_1\vec{x_1} + \cdots + c_k \vec{x_k}\\[NEWLINE][TAB]\vec{x}' &= c_{k + 1}\vec{x_{k + 1}} + \cdots + c_n \vec{x_n}.[NEWLINE]\end{align*}">$$\begin{align*}\vec{x} &= c_1\vec{x_1} + \cdots + c_k \vec{x_k}\\[4px]\vec{x}' &= c_{k + 1}\vec{x_{k + 1}} + \cdots + c_n \vec{x_n}.\end{align*}$$</span></p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: orthogonal decomposition</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> from the previous example, given by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] \right\}.[NEWLINE]$$">$$\begin{align*}X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 3 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 1 \\ 3 \end{array}\right] \right\}.\end{align*}$$</span></p><p class="body-text">Decompose the vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ 3 \end{array}\right][NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 1 \\ 1 \\ 2 \\ 3 \end{array}\right]\end{align*}$$</span></p><p class="body-text">as <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{x} + \vec{x}'">$\vec{v} = \vec{x} + \vec{x}'$</span> for <span class="tex-holder inline-math" data-source-tex="\vec{x} \in X">$\vec{x} \in X$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x}' \in X^\perp">$\vec{x}' \in X^\perp$.</span></p><p class="body-text">We already have an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$,</span> but to produce a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$,</span> we need to extend this to a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> and then run the Gram-Schmidt process on the new vectors. To that end, let&#x2019;s start at the step before we normalized the basis vectors:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] \right\}.[NEWLINE]$$">$$\begin{align*}X = \operatorname{span} \left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] \right\}.\end{align*}$$</span></p><p class="body-text">Extending a basis in general is a little finicky, but nothing too terrible. At least one of the standard basis vectors (and likely most or all of them) will work, so let&#x2019;s try adding the first one. To check if</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right], \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right] \right\}[NEWLINE]$$">$$\begin{align*}\left\{ \left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right], \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right] \right\}\end{align*}$$</span></p><p class="body-text">is a linearly independent set, we just need to show that a matrix with those vectors as rows or columns is invertible, which we can do by showing its determinant is nonzero:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det \left[\begin{array}{cccc} 1& 2& -1& 1 \\ 2& 1& 2& 0 \\ 0& 3& -4& 0 \\ -1& 4& 3& 0 \end{array}\right] &= -\det \left[\begin{array}{ccc} 2& 1& 2 \\ 0& 3& -4 \\ -1& 4& 3 \end{array}\right]\\[NEWLINE][TAB]&= -\left( 2\left( 9 + 16 \right) - \left( -4 - 8 \right) \right)\\[NEWLINE][TAB]&= -124.[NEWLINE]\end{align*}">$$\begin{align*}\det \left[\begin{array}{cccc} 1& 2& -1& 1 \\ 2& 1& 2& 0 \\ 0& 3& -4& 0 \\ -1& 4& 3& 0 \end{array}\right] &= -\det \left[\begin{array}{ccc} 2& 1& 2 \\ 0& 3& -4 \\ -1& 4& 3 \end{array}\right]\\[4px]&= -\left( 2\left( 9 + 16 \right) - \left( -4 - 8 \right) \right)\\[4px]&= -124.\end{align*}$$</span></p><p class="body-text">Now that we&#x2019;ve extended the basis, we just need to run Gram-Schmidt on this last vector. With the notation from the last example, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{y_4} &= \vec{x_4} - \frac{\vec{x_4} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_4} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} - \frac{\vec{x_4} \bullet \vec{y_3}}{\vec{y_3} \bullet \vec{y_3}}\vec{y_3}\\[NEWLINE][TAB]&= \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right] - \frac{1}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] - \frac{2}{30} \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] - \frac{-1}{30} \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c} 2/3 \\ -1/3 \\ -1/3 \\ 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\vec{y_4} &= \vec{x_4} - \frac{\vec{x_4} \bullet \vec{y_1}}{\vec{y_1} \bullet \vec{y_1}}\vec{y_1} - \frac{\vec{x_4} \bullet \vec{y_2}}{\vec{y_2} \bullet \vec{y_2}}\vec{y_2} - \frac{\vec{x_4} \bullet \vec{y_3}}{\vec{y_3} \bullet \vec{y_3}}\vec{y_3}\\[4px]&= \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right] - \frac{1}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] - \frac{2}{30} \left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] - \frac{-1}{30} \left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right]\\[4px]&= \left[\begin{array}{c} 2/3 \\ -1/3 \\ -1/3 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Normalizing this and throwing it into the orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$,</span> we have an orthonormal basis of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left\{ \vec{y_1}, \vec{y_2}, \vec{y_3}, \vec{y_4} \right\} = \left\{ \frac{1}{\sqrt{6}}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right], \frac{1}{\sqrt{6}} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right] \right\}[NEWLINE]$$">$$\begin{align*}\left\{ \vec{y_1}, \vec{y_2}, \vec{y_3}, \vec{y_4} \right\} = \left\{ \frac{1}{\sqrt{6}}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right], \frac{1}{\sqrt{30}}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right], \frac{1}{\sqrt{6}} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right] \right\}\end{align*}$$</span></p><p class="body-text">for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$,</span> and expressing our vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> in it is as simple as taking four dot products.</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{v} &= \left( \vec{v} \bullet \vec{y_1} \right) \vec{y_1} + \left( \vec{v} \bullet \vec{y_2} \right) \vec{y_2} + \left( \vec{v} \bullet \vec{y_3} \right) \vec{y_3} + \left( \vec{v} \bullet \vec{y_4} \right) \vec{y_4}\\[NEWLINE][TAB]&= \frac{0}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] + \frac{21}{30}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{2}{30}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] + \frac{-1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right]\\[NEWLINE][TAB]&= \frac{7}{10}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{1}{15}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] - \frac{1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\vec{v} &= \left( \vec{v} \bullet \vec{y_1} \right) \vec{y_1} + \left( \vec{v} \bullet \vec{y_2} \right) \vec{y_2} + \left( \vec{v} \bullet \vec{y_3} \right) \vec{y_3} + \left( \vec{v} \bullet \vec{y_4} \right) \vec{y_4}\\[4px]&= \frac{0}{6}\left[\begin{array}{c} 1 \\ 2 \\ 0 \\ -1 \end{array}\right] + \frac{21}{30}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{2}{30}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] + \frac{-1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right]\\[4px]&= \frac{7}{10}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{1}{15}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] - \frac{1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">As an orthogonal decomposition,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\frac{7}{10}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{1}{15}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] &\in X\\[NEWLINE][TAB]- \frac{1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right] &\in X^\perp[NEWLINE]\end{align*}">$$\begin{align*}\frac{7}{10}\left[\begin{array}{c} 2 \\ 1 \\ 3 \\ 4 \end{array}\right] + \frac{1}{15}\left[\begin{array}{c} -1 \\ 2 \\ -4 \\ 3 \end{array}\right] &\in X\\[4px]- \frac{1}{6} \left[\begin{array}{c} 2 \\ -1 \\ -1 \\ 0 \end{array}\right] &\in X^\perp\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: orthogonal decomposition</div><p class="body-text">Similar to the previous exercise, let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> given by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right] \right\}.[NEWLINE]$$">$$\begin{align*}X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right] \right\}.\end{align*}$$</span></p><p class="body-text">Decompose the vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right][NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right]\end{align*}$$</span></p><p class="body-text">as <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{x} + \vec{x}'">$\vec{v} = \vec{x} + \vec{x}'$</span> for <span class="tex-holder inline-math" data-source-tex="\vec{x} \in X">$\vec{x} \in X$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{x}' \in X^\perp">$\vec{x}' \in X^\perp$.</span></p></div><p class="body-text">The existence and uniqueness of orthogonal decompositions merits a simpler (and somewhat more descriptive) name:</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: projection</div><p class="body-text">Given a subspace <span class="tex-holder inline-math" data-source-tex="X">$X$</span> of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> and an orthogonal decomposition <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{x} + \vec{x}'">$\vec{v} = \vec{x} + \vec{x}'$,</span> we say that <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$</span> is the <strong>projection</strong> of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> onto <span class="tex-holder inline-math" data-source-tex="X">$X$</span> and write <span class="tex-holder inline-math" data-source-tex="\operatorname{proj}_X\left( \vec{v} \right) = \vec{x}">$\operatorname{proj}_X\left( \vec{v} \right) = \vec{x}$.</span></p></div><p class="body-text">The term <em>projection</em> calls to mind the same sort of shadow-casting idea that we saw in the Gram-Schmidt process example, and in fact, each step in the Gram-Schmidt process is a projection onto the orthogonal complement of the current basis &mdash; more on this in the homework!</p><p class="body-text">As we saw in the proof of orthogonal decomposition, if <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_1}, ..., \vec{x_k} \right\}">$\left\{ \vec{x_1}, ..., \vec{x_k} \right\}$</span> is an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X">$X$,</span> then this projection map is given by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\operatorname{proj}_X\left( \vec{v} \right) = \left( \vec{v} \bullet \vec{x_1} \right) \vec{x_1} + \cdots + \left( \vec{v} \bullet \vec{x_k} \right) \vec{x_k}.[NEWLINE]$$">$$\begin{align*}\operatorname{proj}_X\left( \vec{v} \right) = \left( \vec{v} \bullet \vec{x_1} \right) \vec{x_1} + \cdots + \left( \vec{v} \bullet \vec{x_k} \right) \vec{x_k}.\end{align*}$$</span></p><p class="body-text">That means the projection map is linear, and a simple linear map at that. Extending <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_1}, ..., \vec{x_k} \right\}">$\left\{ \vec{x_1}, ..., \vec{x_k} \right\}$</span> to an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{x_1}, ..., \vec{x_n} \right\}">$\left\{ \vec{x_1}, ..., \vec{x_n} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> the matrix for <span class="tex-holder inline-math" data-source-tex="\operatorname{proj}_X">$\operatorname{proj}_X$</span> expressed in this basis is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right];[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right];\end{align*}$$</span></p><p class="body-text">that is, the top-left <span class="tex-holder inline-math" data-source-tex="k \times k">$k \times k$</span> block is the identity matrix, and every other entry is zero. Expressed in the standard basis, we have a matrix of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]& \hspace{13.3pt} \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right] \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right]^{-1}\\[NEWLINE][TAB]&= \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_n}& - \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{cccccc} \mid& & \mid& \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_k}& \vec{0}& \cdots& \vec{0} \\ \mid& & \mid& \mid& & \mid \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_n}& - \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_k} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_k}& - \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}& \hspace{13.3pt} \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right] \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right]^{-1}\\[4px]&= \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_n} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{c|c} I_k & 0 \\ \hline 0 & 0 \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_n}& - \end{array}\right]\\[4px]&= \left[\begin{array}{cccccc} \mid& & \mid& \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_k}& \vec{0}& \cdots& \vec{0} \\ \mid& & \mid& \mid& & \mid \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_n}& - \end{array}\right]\\[4px]&= \left[\begin{array}{ccc} \mid& & \mid \\ \vec{x_1}& \cdots& \vec{x_k} \\ \mid& & \mid \end{array}\right] \left[\begin{array}{ccc} -& \vec{x_1}& - \\ & \vdots& \\ -& \vec{x_k}& - \end{array}\right].\end{align*}$$</span></p><p class="body-text">This can make computing projections quite a bit easier.</p><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: projections</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right] \right\}">$X = \operatorname{span} \left\{ \left[\begin{array}{c} -1 \\ 1 \\ 1 \\ -1 \end{array}\right], \left[\begin{array}{c} 3 \\ 0 \\ 2 \\ -1 \end{array}\right] \right\}$</span> be the subspace from the previous exercise. Verify that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\operatorname{proj}_X \left( \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right] \right) &= \vec{x}\\[NEWLINE][TAB]\operatorname{proj}_{X^\perp} \left( \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right] \right) &= \vec{x}',[NEWLINE]\end{align*}">$$\begin{align*}\operatorname{proj}_X \left( \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right] \right) &= \vec{x}\\[4px]\operatorname{proj}_{X^\perp} \left( \left[\begin{array}{c} 1 \\ 0 \\ 1 \\ 0 \end{array}\right] \right) &= \vec{x}',\end{align*}$$</span></p><p class="body-text">where <span class="tex-holder inline-math" data-source-tex="\vec{x} + \vec{x}'">$\vec{x} + \vec{x}'$</span> was the orthogonal decomposition from before.</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>