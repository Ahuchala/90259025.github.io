### nav-buttons

When learning math, it's nice to have an idea of the big picture as often as possible. The definitions and theorems and examples can feel like slogging through tall grass, and without a clear picture of where you're going and why, it can all seem a little pointless. First, let's run down what we'll be building over the entire course.

Throughout all of calculus I and II, the easiest type of functions to work with are polynomials. Limits, derivatives, and integrals are all easy when the function you're operating on is a polynomial, since all three split across addition, so you can take them term-by-term. Moreover, every term of a polynomial is a continuous function, so limits are as easy as plugging in the number you're limiting to, and derivatives and integrals just leverage the power rule.

It goes without saying that not all functions are polynomials. $\sin(x)$, $\ln(x)$, and $2^x$ are all substantially more complicated, and there is no way to write any as a sum of monomials. But we're not as far off as you might think --- let's take a close look at $\sin(x)$.

### desmos taylorSeries

Drag the blue point around, and try increasing $N$. The blue function is always a polynomial, and it is almost exactly equal to $\sin(x)$, as long as you only look near the blue point. As $N$ increases, so does the length of the interval where the two functions agree. It will turn out that as $N \to \infty$, that interval becomes infinitely long, and the functions become exactly equal to one another. So $\sin$ may not be a polynomial, but it is a *limit* of polynomials. This is a very different kind of limit from the one we're used to, though --- rather than a sequence of numbers approaching another number, what we have here is a sequence of *functions* approaching another function. The upshot of this intricate construction is that many of the best properties of polynomials will carry over to their limits, so working with $\sin$ --- and more importantly, other more complicated functions --- will be made much easier.

We'll spend the first half of the course building tools and machinery in order to understand the ideas of adding together infinitely many functions that can sometimes converge. In the second half, we'll specialize to the specific kind of sums that approximate functions, like we just did with $\sin$. While many of the topics we cover will be interesting in their own right, it's good to keep this rough roadmap in mind as we move from one to the next.



## Sequences

To talk about the complex behavior of the polynomials we just saw, we need to build the groundwork of **sequences**. As with every topic, we'll start with a definition, give some examples and exercises, and then talk about some properties.



### def "sequence"
	
	A **sequence** is an infinite list of numbers, written
	
	$$
		(a_1, a_2, a_3, ...).
	$$
	
	We can write this concisely as $(a_n)_{n = 1}^\infty$, or more commonly, just $(a_n)$. The $a_n$ are called **terms** of the sequence, and $n$ is called the **index variable**.
	
	The alternate notation $\{a_n\}$ is often used, including in our textbook. In general, though, curly braces denote collections of numbers where order and duplicates don't matter, and parentheses denote collections where they do matter, like in ordered pairs $(x, y)$. Since order and duplicates do matter in sequences, we'll use the parentheses notation in these notes.
	
###



### ex "writing out a sequence from a formula"
	
	Let $a_n = 2n$, $b_n = n^2 + 1$, and $c_n = (n - 1)!$. Write out the first five terms of the sequences $(a_n)$, $(b_n)$, and $(c_n)$.
	
	Let's start with $(a_n)$. Unless otherwise specified, sequences start with $n = 1$, so the first term is $a_1 = 2 \cdot 1 = 2$. Plugging in $n = 2$ through $n = 5$, we find that
	
	$$
		(a_n) = (2, 4, 6, 8, 10, ...).
	$$
	
	Similarly, we have
	
	$$
		(b_n) = (2, 5, 10, 17, 26, ...).
	$$
	
	Now let's look at $(c_n)$. That exclamation point is a <dfn>factorial</dfn>: to take the factorial of a positive integer $n$, multiply together all the integers less than or equal to $n$. For example,
	
	$$
		1! &= 1.
		
		2! &= 2 \cdot 1 = 2.
		
		3! &= 3 \cdot 2 \cdot 1 = 6.
		
		4! &= 4 \cdot 3 \cdot 2 \cdot 1 = 24.
	$$
	
	We also define $0! = 1$. That might seem weird, but we'll be making heavy use of factorials later in the course, and we'll really want it to be defined that way. Putting everything together, we have
	
	$$
		(c_n) = (1, 1, 2, 6, 24, ...).
	$$
	
###



A formula in terms of $n$ that defines a sequence --- like all three in the previous example --- is called an **explicit formula**. We can also define sequences **recursively** by writing $a_n$ in terms of $a_{n - 1}$, $a_{n - 2}$, and so on. Let's take a look at how these work.



### ex "a sequence defined recursively"
	
	Let $a_1 = 1$ and $a_n = 2a_{n - 1}$. Write out the first five terms of $(a_n)$ and find an explicit formula.
	
	Whenever a sequence is defined recursively, there must be at least one term that's given exactly, so that we have a place to start from. Here, that's $a_1 = 1$. From here, let's use the recursion to find $a_2$. Plugging in $n = 2$ to $a_n = 2a_{n - 1}$, 
	
	$$
		a_2 = 2a_1 = 2 \cdot 1 = 2.
	$$
	
	Now with $n = 3$, we have
	
	$$
		a_3 = 2a_2 = 2 \cdot 2 = 4.
	$$
	
	Continuing in this way,
	
	$$
		(a_n) = (1, 2, 4, 8, 16, ...).
	$$
	
	To write this as an explicit formula, we need to know what $a_n$ is in terms of only $n$, and not any previous $a_i$. Here, that's not too hard: each term is a power of $2$. It's tempting to say that the formula is $a_n = 2^n$, but let's check that idea: with $n = 1$, we get $a_1 = 2^1 = 2$ and $a_2 = 2^2 = 4$. The pattern matches, but the power is one too high on each. To fix that, we'll just take one off to get $a_n = 2^{n - 1}$.
	
	To check our answer, let's plug it into the recurrence. We have $a_1 = 2^{1 - 1} = 1$ and
	
	$$
		a_n &= 2a_{n - 1}
		
		&= 2(2^{n - 1})
		
		&= 2^n,
	$$
	
	which is exactly correct!
	
###

### exc "sequences"
	
	1. The Fibonacci numbers are a very famous sequence that begin with $F_1 = 1$ and $F_2 = 1$. After this, every Fibonacci number is the sum of the previous two. Find a recursive formula for the Fibonacci sequence $(F_n)$.
	
	2. Find recursive formulas for each of the three sequences in the first example.
	
	3. Let $(d_n)$ be the sequence defined by $d_1 = 0$ and $d_n = d_{n - 1} + \frac{1}{2^n}$. Find an explicit formula for $(d_n)$ by writing out terms and finding a pattern, and verify your answer by plugging it back into the recurrence.
	
	**Solutions:**
	
	1. Since every $F_n$ is the sum of the previous two, $F_n = F_{n - 1} + F_{n - 2}$. A full recursive formula is
	
	$$
		F_1 = 1, F_2 = 1, F_n = F_{n - 1} + F_{n - 2}.
	$$
	
	2. Every $a_n$ is obtained from the previous by adding $2$. Therefore, we have
	
	$$
		a_1 = 2, a_n = a_{n - 1} + 2.
	$$
	
	The $b_n$ are a little more complicated. Looking at the differences between successive terms, we have
	
	$$
		5 - 2 &= 3
		
		10 - 5 &= 5
		
		17 - 10 &= 7
		
		26 - 17 &= 9.
	$$
	
	We seem to be adding $2$ more every time, starting with $2$. So $b_2 = 2\cdot 1 + 1 + b_1$, $b_3 = 2\cdot 2 + 1 + b_2$, and so on. The pattern winds up looking like
	
	$$
		b_1 = 2, b_n = 2(n - 1) + 1 + b_{n-1}.
	$$
	
	An alternative way to arrive at the same solution is to look at $b_n - b_{n - 1}$. Using the explicit formula, this becomes
	
	$$
		n^2 + 1 - ((n-1)^2 + 1) &= n^2 - (n-1)^2
		
		&= n^2 - (n^2 - 2n + 1)
		
		&= 2n - 1.
	$$
	
	Therefore, $b_n = b_{n - 1} + 2n - 1$.
	
	With $c_n$, rather than adding terms, we're multiplying to get from one to the next. For example, the change from $3!$ to $4!$ is multiplication by $4$. We wind up with the formula
	
	$$
		c_1 = 1, c_n = (n-1)c_{n - 1}.
	$$
	
	3. Let's write out the first five terms and see if we can find a pattern.
	
	$$
		(d_n) = \left(0, \frac{1}{4}, \frac{3}{8}, \frac{7}{16}, \frac{15}{32}, ...\right).
	$$
	
	At every step, we seem to be multiplying the denominator by $2$, and the numerator is always one less than necessary for the fraction to equal $\frac{1}{2}$; in other words, it's $2^{n-1} - 1$. We might guess that the explicit formula comes out to
	
	$$
		d_n = \frac{2^{n - 1} - 1}{2^n}.
	$$
	
	Let's verify this. We have $d_1 = \frac{2^0}{2^1} = 0$, which is correct, and
	
	$$
		d_{n - 1} + \frac{1}{2^n} &= \frac{2^{n - 2} - 1}{2^{n - 1}} + \frac{1}{2^n}
		
		&= \frac{2^{n - 1} - 2}{2^n} + \frac{1}{2^n}
		
		&= \frac{2^{n - 1} - 1}{2^n}
		
		&= d_n,
	$$
	
	which is also exactly what we wanted.
	
###



There are two particular types of sequences that crop up often, and it will help to give them names so we can talk about about them more easily.



### def "arithmetic and geometric sequences"
	
	An **arithmetic** sequence is one for which the difference between any two consecutive terms is the same. A **geometric** sequence is one for which the ratio between any two consecutive terms is the same.
	
	Note: the word *arithmetic* is pronounced differently here than it usually is --- the stress is on the third syllable instead of the second.
	
###



### ex "arithmetic and geometric sequences"
	
	The sequence
	
	$$
		(a_n) = (1, -2, -5, -8, -11, -14, ...)
	$$
	
	is arithmetic, since the difference between every pair of consecutive terms is exactly $-3$. Equivalently, we could write $a_1 = 1$ and $a_n = a_{n - 1} - 3$. On the other hand, the sequence
	
	$$
		(b_n) = (3, 6, 12, 24, 48, 96, ...)
	$$
	
	is geometric, since the ratio between every pair of consecutive terms is exactly $2$. Equivalently, $b_1 = 3$ and $b_n = 2b_{n - 1}$.
	
###

### exc "arithmetic and geometric sequences"
	
	Find explicit formulas for $(a_n)$ and $(b_n)$ and check that they're correct.
	
	**Solution:**
	
	We have
	
	$$
		a_1 &= 1
		
		a_2 &= 1 - 3
		
		a_3 &= 1 - 3 - 3
		
		a_4 &= 1 - 3 - 3 - 3.
	$$
	
	every $a_n$ begins with a $1$ and then subtracts $n - 1$ $3$s, so an explicit formula is $a_n = 1 - 3(n-1)$. Similarly, every $b_n$ begins with a $3$ and then multiplies $n - 1$ $2$s, so we get $b_n = 2(3^{n - 1})$.
	
	To check that these formulas are correct, we verify the initial conditions and the recurrence. For $a_n$, $a_1 = 1 - 3(0) = 0$, which is correct, and
	
	$$
		a_{n - 1} - 3 &= 1 - 3((n-1) - 1) - 3
		
		&= 1 - 3(n - 1)
		
		&= a_n.
	$$
	
	Similarly, $b_1 = 2$ and $b_n = 2b_{n - 1}$.
	
###



## Convergence

Although they're interesting in their own right, we need sequences because of their ability to **converge**. The idea is very similar to the limit of a function $f(x)$ as $x \to \infty$, so we already have a lot of intuition for how the limit of a sequence might work. For example,

$$(1, 2, 3, 4, 5, ...) \to \infty$$.

$$\left(1 + \left(-\frac{1}{2}\right)^n \right) = \left(\frac{1}{2}, \frac{5}{4}, \frac{7}{8}, \frac{17}{16}, \frac{31}{32}, ... \right) \to 1$$.

$$(1, -1, 1, -1, 1, ...)$$ does not converge.

Chances are good that when you first learned about limits, you were never given a precise definition. This is for good reason: limits are usually fairly easy to find by inspection, as in the previous three examples; the actual definition is somewhat finicky and precise; and calculus I already has enough concepts that are difficult to digest. Now, though, we need to know that exact definition for sequences, and calculus III is high-level enough that it's reasonable to introduce.

Let's begin with the informal definition, which is a good way to think of limits of sequences in practice.



### def "the limit of a sequence (informal)"
	
	Let $(a_n)$ be a sequence. If the terms $a_n$ get closer and closer to some finite number $a$ as $n$ gets larger and larger, then we say that $(a_n)$ **converges** to $a$, and we write any of the following:
	
	$$(a_n) \to a$$.
	
	$$\lim_{n \to \infty} (a_n) = a$$.
	
	$$\lim (a_n) = a$$ (since the only limit we can take of a sequence is when $n \to \infty$).
	
	If $(a_n)$ does not converge, then we say it **diverges**.
	
###



The informal parts of this definition are the phrases "closer and closer" and "larger and larger". Let's nail those down.



### def "the limit of a sequence (formal)"
	
	Let $(a_n)$ be a sequence. If there is a number $a$ such that for all real numbers $\varepsilon > 0$, there is a positive integer $N$ so that $|a_n - a| < \varepsilon$ when $n \geq N$, then we say that $(a_n)$ converges to $a$.
	
###



Let's unpack this carefully. First, we start with a candidate $a$ that we think is the limit of $(a_n)$. To check if that's the case, we pick a (small) number $\varepsilon > 0$. We need to find a point in the sequence after which **every** term is within $\varepsilon$ of $a$. If we can always find such an $N$ no matter how small $\varepsilon$ is, then the sequence converges to $a$.



### ex "the formal definition of a limit"
	
	The sequence $(a_n)$ defined by
	
	$$
		a_n = 1 + \left(-\frac{1}{2}\right)^n
	$$
	
	converges to $1$. Find $N$ such that $|a_n - 1| < \varepsilon$ for $\varepsilon = 1$, $\varepsilon = .1$, and $\varepsilon = .001$.
	
	Let's take a look at that inequality. We have
	
	$$
		|a_n - 1| &= \left| 1 + \left(-\frac{1}{2}\right)^n - 1 \right|
		
		&= \left| \left(-\frac{1}{2}\right)^n\right|
		
		&= \left(\frac{1}{2}\right)^n
		
		&= \frac{1}{2^n}.
	$$
	
	Therefore, when $\varepsilon = 1$, we need to find an $N$ so that whenever $n \geq N$, $\frac{1}{2^n} < 1$. Since $\frac{1}{2^n}$ decreases as $n$ increases and $\frac{1}{2^1} = \frac{1}{2}$, we can just take $N = 1$.
	
	For $\varepsilon = .1$, we need to figure out when $\frac{1}{2^n} < .1$. Plugging in a few values of $n$ tells us the first time that happens is when $n = 4$, and since larger values of $n$ only make $\frac{1}{2^n}$ even smaller, we can take $N = 4$.
	
	Finally, the first $n$ that makes $\frac{1}{2^n} < .001$ is $n = 10$, so we take $N = 10$.
	
	If we plot this sequence like a function, putting values of $n$ on the horizontal axis and $a_n$ on the vertical axis, convergence means that for any thin band around the horizontal line at height $L$, we can find an $N$ so that all the points to the right of $N$ stay in that band.
	
	### desmos epsilonDefinitionOfConvergence
	
###

### exc "the formal definition of a limit"
	
	The sequence $(b_n)$ defined by
	
	$$
		b_n = \frac{1}{n}
	$$
	
	converges to $0$. Find $N$ such that $|b_n| < \varepsilon$ for $\varepsilon = 1$, $\varepsilon = .1$, and $\varepsilon = .001$.
	
	**Solution:** When $\varepsilon = 1$, the equation becomes
	
	$$
		\left| \frac{1}{n} \right| < 1.
	$$
	
	Since $\frac{1}{n} > 0$, the absolute value disappears, and so we find that
	
	$$
		\frac{1}{n} &< 1
		
		n &> 1.
	$$
	
	Therefore, we can take $N = 2$. When $\varepsilon = .1$, the solution becomes
	
	$$
		n > \frac{1}{.1} = 10,
	$$
	
	so a solution is $N = 11$. Similarly, with $\varepsilon = .001$, a solution is $N = 1001$.
	
###



## Properties of Sequences

When we have a sequence $(a_n)$ defined explicitly (i.e. $a_n = f(n)$ for some function $f$), **and** when $\lim_{x \to \infty} f(x)$ exists, then the limit of the sequence exists and is the same as the limit of the function. For example, let's look at the sequence $(r^n)$. When $0 \leq r < 1$, $r^x \to 0$ as $x \to \infty$, so $(r^n) \to 0$. When $r = 1$, $r^x = 1$ for all $x$, so $(r^n) \to 1$. And when $r > 1$, $r^x \to \infty$ as $x \to \infty$, so the same is true of $(r^n)$. Therefore, $(r^n)$ converges for $0 \leq r \leq 1$ and diverges for $r > 1$.

However, if the limit of the defining function does not exist, we need to be more careful. For example, the sequence $a_n = \cos(2\pi n)$ agrees with the function $f(x) = \cos(2\pi x)$, but while the function alternates between $1$ and $-1$ and therefore has no limit to $\infty$, the sequence only has integer values of $n$, meaning $a_n = 1$ for all $n$. Therefore, the sequence converges to 1.

This is the first example of many of a sequence that converges only sometimes, depending on a parameter (here, that parameter is $r$). This sort of construction comes up so often that it will end up being one of the main objects we study in the course.

Many of the rules of limits for functions also apply to sequences, so we can often skip annoying computations.



### prop "properties of limits"
	
	Suppose $(a_n) \to a$ and $(b_n) \to b$. Let $c$ be a constant. Then:
	
	1. $$(c) \to c$$ (i.e. a sequence of constants converges to that constant).
	
	2. $$(c \cdot a_n) \to c \cdot a$$.
	
	3. $$(a_n + b_n) \to a + b$$.
	
	4. $$(a_n b_n) \to ab$$.
	
	5. If $$b \neq 0$$ and every $$b_n \neq 0$$, then $$\left(\frac{a_n}{b_n}\right) \to \frac{a}{b}$$.
	
	6. If $$f(x)$$ is continuous at $$x = a$$, then $$(f(a_n)) \to f(a)$$.
	
###



### exc "properties of limits"
	
	Find the limits of the following sequences.
	
	1. $$(n!)$$.
	
	2. $$\left(1 - \frac{1}{n}\right)$$.
	
	3. $$\left(\sin \left( \frac{n}{2^n} \right) \right)$$.
	
	**Solutions:**
	
	1. As $n \to \infty$, $n!$ becomes very large very fast. Increasing $n$ by just $1$ increases the term by a factor of $n$. Therefore, $(n!) \to \infty$.
	
	2. This sequence is matched by the function $1 - \frac{1}{x}$. As $x \to \infty$, $\frac{1}{x} \to 0$, so $1 - \frac{1}{x} \to 1$. Therefore, the limit of the sequence is $1$.
	
	3. We can use the same strategy as the previous question, but this time we need to apply L'H&#244;pital's rule. We have
	
	$$
		\lim_{x \to \infty} \frac{x}{2^x} = \lim_{x \to \infty} \frac{1}{2^x \ln(2)} = 0.
	$$
	
	Therefore, the limit of the sequence is $\sin(0) = 0$.
	
###



Just like with functions, there is a Squeeze theorem for sequences. While the function version is often overlooked, though, the sequence version is critically important.



### thm "The Squeeze Theorem"
	
	Let $(a_n)$, $(b_n)$, and $(c_n)$ be sequences such that for all $n$, $a_n \leq b_n \leq c_n$. If $(a_n) \to b$ and $(c_n) \to b$, then $(b_n)$ converges and $(b_n) \to b$.
	
###



### ex "the Squeeze theorem"
	
	Find $\lim \left( \frac{\cos(n)}{n} \right)$.
	
	It's not immediately clear that this converges at all. To start, remember that $-1 \leq \cos(n) \leq 1$, so
	
	$$
		-\frac{1}{n} \leq \frac{\cos(n)}{n} \leq \frac{1}{n}.
	$$
	
	Now we know that both of the outer sequences converge to zero, so since they share a limit, the Squeeze theorem guarantees that $\lim \left( \frac{\cos(n)}{n} \right) = 0$ too.
	
	### desmos squeezeTheorem
	
###



### exc "the Squeeze theorem"
	
	We already understand the behavior of the sequence $(r^n)$ when $r \geq 0$: it converges to zero for $0 \leq r < 1$, to one for $r = 1$, and it diverges for $r > 1$. Use the Squeeze theorem to classify the behavior of $(r^n)$ for negative $r$.
	
	**Solution:** When $r < 0$, $r^n$ is positive for even $n$ and negative for odd $n$. To make our notation a little simpler, let's let $r > 0$ and consider the sequence $(-r)^n$. When $0 < r < 1$, we can squeeze $(-r)^n$ in the following manner:
	
	$$
		-(r^n) \leq (-r)^n \leq r^n.
	$$
	
	Since both outer terms limit to zero, so does $(-r)^n$.
	
	We already know how the sequence $(-1)^n$ behaves: it flip-flops between $1$ and $-1$, never settling down and therefore not converging. Finally, for $(-r)^n$ when $r > 1$, it alternates between $r^n$ and $-(r^n)$. Neither of those converges, so $(-r)^n$ cannot converge either.
	
	In total, the series $(r^n)$ converges to zero when $-1 < r < 1$, converges to one when $r = 1$, and diverges otherwise.
	
###



There's one final point to touch on before we leave this section. We can often tell when a sequence converges, even if we don't know what it converges to.



### def "bounded sequence"
	
	A sequence $(a_n)$ is **bounded above** if there is some $M$ so that $a_n \leq M$ for all $n$. Similarly, $(a_n)$ is **bounded below** if there is some $M$ so that $(a_n) \geq M$ for all $n$.
	
###



### def "monotone sequence"
	
	A sequence $(a_n)$ is **monotone increasing** if $a_{n + 1} \geq a_n$ for all $n$, and **monotone decreasing** if $a_{n + 1} \leq a_n$ for all $n$.
	
###



For example, $(n^2)$ is monotone increasing and bounded below but not above, $(-\ln(n))$ is monotone decreasing and bounded above but not below, and $(\sin(n))$ is not monotone increasing or decreasing, but it is bounded above and below.

If a sequence is monotone increasing and bounded above, then its behavior is very constrained. It must eventually settle down to some number --- it cannot run off to infinity, since it's bounded above, or negative infinity, since it's increasing. And it can't fluctuate and avoid settling down to anything in the way $(\sin(n))$ does, since it is increasing. The same logic applies to sequences that are monotone decreasing and bounded below.



### thm "The Monotone Convergence Theorem"
	
	Every sequence that is monotone increasing and bounded above, or monotone decreasing and bounded below, converges.
	
###



### exc "the Monotone Convergence theorem"
	
	Let $(a_n)$ be a sequence defined recursively by $a_1 = 2$ and
	
	$$
		a_n = \frac{a_{n - 1}}{2} + \frac{1}{2a_{n - 1}}
	$$
	
	for $n \geq 2$.
	
	First, show that $(a_n)$ converges using the MCT. Then find the number it converges to. (Hint: take the limit of both sides of the recurrence.)
	
	**Solution:** We want to show that $a_n$ is either bounded above and increasing or bounded below and decreasing. Plugging in a few values of $n$, we get
	
	$$
		a_2 &= \frac{2}{2} + \frac{1}{4} = \frac{5}{4}
		
		a_3 &= \frac{5/4}{2} + \frac{1}{5/2} = \frac{5}{8} + \frac{2}{5} = \frac{41}{40}.
		
		a_4 &= \frac{41/40}{2} + \frac{1}{41/20} = \frac{41}{80} + \frac{20}{41} = \frac{3281}{3280}.
	$$
	
	So far all the terms are decreasing --- hopefully, the sequence is always decreasing and also bounded below. Let's start with bounded --- it certainly seems to be bounded below by $1$, so let's see if we can show that. We'll solve the inequality $a_n \geq 1$ and hope to find that it's always true.
	
	$$
		a_n &\geq 1
		
		\frac{a_{n - 1}}{2} + \frac{1}{2a_{n - 1}} & \geq 1
		
		\frac{a_{n - 1}^2 + 1}{2a_{n - 1}} & \geq 1
		
		a_{n - 1}^2 + 1 & \geq 2a_{n - 1}
		
		a_{n - 1}^2 - 2a_{n - 1} + 1 & \geq 0
		
		\left( a_{n - 1} - 1 \right)^2 & \geq 0.
	$$
	
	Since any number squared is positive, this is always true. Now let's show that the sequence is decreasing, which amounts to showing that $a_n \leq a_{n - 1}$. Expanding, we have
	
	$$
		a_n & \leq a_{n - 1}
		
		\frac{a_{n - 1}}{2} + \frac{1}{2a_{n - 1}} & \leq a_{n - 1}
		
		\frac{a_{n - 1}^2 + 1}{2a_{n - 1}} & \leq a_{n - 1}
		
		a_{n - 1}^2 + 1 & \leq 2a_{n - 1}^2
		
		1 & \leq a_{n - 1}^2
		
		a_{n - 1} & \geq 1 \text{ or } a_{n - 1} & \leq -1.
	$$
	
	Since $a_{n - 1} \geq 1$, this is always the case. Therefore, the MCT applies, the the sequence converges.
	
	To find the limit, let's follow the hint and take the limit of both sides. If $\lim_{n \to \infty} a_n = a$, then
	
	$$
		a_n &= \frac{a_{n - 1}}{2} + \frac{1}{2a_{n - 1}}
		
		\lim_{n \to \infty} a_n &= \lim_{n \to \infty} \left( \frac{a_{n - 1}}{2} + \frac{1}{2a_{n - 1}} \right)
		
		a &= \frac{a}{2} + \frac{1}{2a}
		
		a &= \frac{a^2 + 1}{2a}
		
		2a^2 &= a^2 + 1
		
		a^2 &= 1
		
		a &= \pm 1.
	$$
	
	Since $a_n \geq 1$ for all $n$, we must have that $a \geq 1$, so $a = 1$.
	
###


	
### nav-buttons