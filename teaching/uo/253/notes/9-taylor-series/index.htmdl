### nav-buttons

By now, we've built up a robust theory of series. We know when and where they converge, and in a few special cases, we even know what they converge to. In this section, we'll extend those few special cases to almost every power series we've seen, and by the end, we'll be able to pass from functions to power series and back almost effortlessly.

Let's return to an old friend: $\frac{1}{1 - x}$. We already know that when $|x| < 1$, it's equal to $\sum_{n = 0}^\infty x^n$, but the way we found that was by directly taking a limit of partial sums and expressing $1 + x + x^2 + \cdots + x^k$ as $\frac{1 - x^{k + 1}}{1 - x}$. That method was specific to this particular series and doesn't really help with any others. Instead, let's try to find series like that in general, without any fancy tricks.

Take a power series $\sum_{n = 0}^\infty c_n (x - a)^n$ centered at $x = a$, and suppose that the series converges to a function $f(x)$ on some interval $(a - R, a + R)$. With the logic we used at the end of the previous section, we can extract the derivatives of $f$ from the coefficients of the series. Specifically, we have

$$
	f(a) &= c_0
	
	f'(a) &= c_1
	
	f''(a) &= 2 \cdot c_2
	
	f'''(a) &= 3 \cdot 2 \cdot c_3
	
	f''''(a) &= 4 \cdot 3 \cdot 2 \cdot c_4
	
	& \ \ \vdots
	
	f^{(n)}(a) &= n!c_n.
$$

From this perspective, if we know a power series representation of a function $f$, then we can find any derivative of $f$ from the coefficients. But we really want to go the other way: we want to *find* series representations from a function. To get at that direction, let's solve all of those equations for the $c_n$:

$$
	c_0 &= \frac{f^{(0)}(a)}{0!} = f(a)
	
	c_1 &= \frac{f^{(1)}(a)}{1!} = f'(a)
	
	c_2 &= \frac{f^{(2)}(a)}{2!} = \frac{f''(a)}{2}
	
	c_3 &= \frac{f^{(3)}(a)}{3!} = \frac{f'''(a)}{3!}
	
	c_0 &= \frac{f^{(4)}(a)}{4!}
	
	& \ \ \vdots
	
	c_n &= \frac{f^{(n)}(a)}{n!}.
$$

What we've come to is the idea that underpins the entire section, and to a large extent the course: if $f$ has a power series representation centered at $x = a$, then there is only one possibility for what it can be, since power series representations are unique, and we've just written down what that one possibility is. We'll be talking quite a lot about these things, and they deserve their own name.



### def "Taylor series"
	
	Let $f(x)$ be a function that is infinitely differentiable. The **Taylor series** of $f$ centered at $a$ is the power series
	
	$$
		\sum_{n = 0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n.
	$$
	
	A Taylor series centered at $0$ is called a **Maclaurin series**. The partial sums of a Taylor or Maclaurin series are called the **Taylor** or **Maclaurin polynomials** for $f$ centered at $a$.

###



Critically, this definition says nothing about convergence: the Taylor series for a function </strong>does not necessarily converge to that function</strong>. We'll see examples of this a little later, but the takeaway is that we need to verify convergence to show that a Taylor series actually does what it's supposed to.



### ex "finding a Maclaurin series"
	
	Find the Maclaurin series for $\sin(x)$ and determine its interval of convergence. Find the first six Maclaurin polynomials.
	
	The Maclaurin series is just the Taylor series centered at $0$, so it is $\sum_{n = 0}^\infty c_n x^n$, where
	
	$$
		c_n = \frac{1}{n!} \frac{d^n}{dx^n} [\sin(x)]|_{x = 0}.
	$$
	
	Okay, so what is that massive derivative? Let's list out a few derivatives of $\sin(x)$ to remind ourselves of the pattern.
	
	$$
		\frac{d}{dx}[\sin(x)] &= \cos(x)
		
		\frac{d^2}{dx^2}[\sin(x)] &= -\sin(x)
		
		\frac{d^3}{dx^3}[\sin(x)] &= -\cos(x)
		
		\frac{d^4}{dx^4}[\sin(x)] &= \sin(x).
	$$
	
	When we plug in $x = 0$, $\sin(0) = 0$ while $\cos(0) = 1$. All of the even derivatives have a $\sin$ in them, so they all vanish. The remaining terms alternate in sign as the derivative flips from $\cos(x)$ to $-\cos(x)$. In total, we wind up with
	
	$$
		x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots.
	$$
	
	It's pretty awkward to write those coefficients as a series if we try to include the missing terms. Instead, let's write this as an alternating series by changing the power on $x$:
	
	$$
		\sum_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!}.
	$$
	
	The $2n + 1$ is just a nice way of expressing all the odd numbers.
	
	Next, let's determine where this series converges. The ratio test is often the best bet for Taylor series, since they're full of factorials and exponents. Evaluating it on this series, we have
	
	$$
		\lim_{n \to \infty} \left| \frac{(-1)^{n + 1} \frac{x^{2(n + 1) + 1}}{(2(n + 1) + 1)!}}{(-1)^n \frac{x^{2n + 1}}{(2n + 1)!}} \right| &= \lim_{n \to \infty} \left| -\frac{x^{2(n + 1) + 1}(2n + 1)!}{x^{2n + 1}(2(n + 1) + 1)!} \right|
		
		&= \lim_{n \to \infty} \left| \frac{x^{2n + 3}(2n + 1)!}{x^{2n + 1}(2n + 3)!} \right|
		
		&= \lim_{n \to \infty} \left| \frac{x^2}{(2n + 3)(2n + 2)} \right|
		
		&= \lim_{n \to \infty} \left| \frac{x^2}{(2n + 3)(2n + 2)} \right|
		
		&= 0.
	$$
	
	Therefore, the series converges everywhere.
	
	The Maclaurin polynomials are just partial sums of this series. Denoting the first six polynomials as $p_0, p_1, ..., p_5$, every $p_N$ should have a highest term of $x^N$. Therefore, we have
	
	$$
		p_0 &= 0
		
		p_1 &= x
		
		p_2 &= x
		
		p_3 &= x - \frac{x^3}{3!}
		
		p_4 &= x - \frac{x^3}{3!}
		
		p_5 &= x - \frac{x^3}{3!} + \frac{x^5}{5!}.
	$$
	
	Plotting these polynomials shows that they really do seem to converge to $\sin$: plotted in purple is $p_N$, and increasing $N$ increases the interval where $p_N$ is almost identical to $\sin$. You can also change $a$ to see the effect recentering has.
	
	### desmos taylorSeries

###

### exc "finding Taylor series"
	
	Find the following Taylor series. For each, determine the interval of convergence and find the first three Taylor polynomials.
	
	1. A Taylor series for $$\ln(x)$$ centered at $x = 1$.
	
	2. A Taylor series for $$x^2 + 4x + 4$$ centered at $x = 2$.
	
	3. A Taylor series for $$\frac{1}{1 + x}$$ centered at $x = 1$.
	
	4. A Maclaurin series for $$e^x$$.
	
	**Solutions:**
	
	1. Let's start by making a list of the first few derivatives. We have
	
	$$
		\frac{d}{dx}[\ln(x)] &= x^{-1}
		
		\frac{d^2}{dx^2}[\ln(x)] &= -x^{-2}
		
		\frac{d^3}{dx^3}[\ln(x)] &= 2x^{-3}
		
		\frac{d^4}{dx^4}[\ln(x)] &= -3 \cdot 2 x^{-4}
		
		\frac{d^5}{dx^5}[\ln(x)] &= 4 \cdot 3 \cdot 2 x^{-5}
	$$
	
	Plugging in $x = 1$ erases the factor involving $x$, so we wind up with
	
	$$
		c_0 &= \frac{\ln(1)}{0!} = 0
		
		c_1 &= \frac{1}{1!} = 1
		
		c_2 &= \frac{-1}{2!} = -\frac{1}{2}
		
		c_3 &= \frac{2}{3!} = \frac{1}{3}
		
		c_4 &= \frac{-3 \cdot 2}{4!} = -\frac{1}{4}
		
		c_5 &= \frac{4 \cdot 3 \cdot 2}{5!} = \frac{1}{5}.
	$$
	
	From this, we can see that $c_n = (-1)^{n + 1} \frac{1}{n}$, so our series is
	
	$$
		\sum_{n = 1}^\infty (-1)^{n + 1} \frac{(x - 1)^n}{n}.
	$$
	
	The first three polynomials have degree zero through two, and they are
	
	$$
		0, x - 1, (x - 1) - \frac{(x - 1)^2}{2}.
	$$
	
	To determine the interval of convergence, we can apply the ratio test as usual. After simplifying, we have
	
	$$
		r = |x - 1|,
	$$
	
	so the interval of convergence is at least $(0, 2)$. Checking the endpoints, $x = 0$ produces the Harmonic series but with all negative terms, which diverges, while $x = 2$ results in the Alternating Harmonic series, which converges. In total, the interval of convergence is $(0, 2]$.
	
	2. This is quite a bit simpler: we have $f(2) = 16$, $f'(2) = 8$, $f''(2) = 2$, and all further derivatives of $f$ are zero. The coefficients are therefore $c_0 = 16$, $c_1 = 8$, and $c_2 = 1$, so the series is
	
	$$
		16 + 8(x - 2) + (x - 2)^2.
	$$
	
	This makes sense --- if you expand this out, it's equal to $x^2 + 4x + 4$! The interval of convergence is $(-\infty, \infty)$, since a finite series can never diverge, and the first three Taylor polynomials are $16$, $16 + 8(x - 2)$, and $16 + 8(x - 2) + (x - 2)^2$.
	
	3. We already know a series representation of this, but to get a Taylor series, we just take derivatives. We have
	
	$$
		\frac{d}{dx}\left[ \frac{1}{1 + x} \right] &= -(1 + x)^{-2}
		
		\frac{d^2}{dx^2}\left[ \frac{1}{1 + x} \right] &= 2(1 + x)^{-3}
		
		\frac{d^3}{dx^3}\left[ \frac{1}{1 + x} \right] &= -6(1 + x)^{-4}
		
		\frac{d^4}{dx^4}\left[ \frac{1}{1 + x} \right] &= 24(1 + x)^{-5}
	$$
	
	Plugging in $1$ and extrapolating, we have $c_n = \frac{(-1)^n}{2^{n + 1}}$, since the $n!$ cancels. The series is just
	
	$$
		\sum_{n = 0}^\infty \frac{(-1)^n}{2^{n + 1}} (x - 1)^n = \frac{1}{2} \sum_{n = 0}^\infty \frac{(-1)^n}{2^n} (x - 1)^n.
	$$
	
	We can apply the ratio test, but the root test is even easier, since we just have $r = \lim_{n \to \infty} \frac{|x - 1|}{2}$. Therefore, the series converges on at least $(-1, 3)$. Plugging in endpoints, we have that $x = -1$ and $x = 3$ both produce divergent series, so $(-1, 3)$ is the total interval of convergence. The first three Taylor polynomials are just $\frac{1}{2}$, $\frac{1}{2} - \frac{1}{4} (x - 1)$, and $\frac{1}{2} - \frac{1}{4} (x - 1) + \frac{1}{8} (x - 1)^2$.
	
	4. Since every derivative of $e^x$ is just $e^x$, plugging in $x = 0$ results in $1$ for every term. Therefore, the Maclaurin series is just
	
	$$
		\sum_{n = 0}^\infty \frac{x^n}{n!}.
	$$
	
	Applying the ratio test gives $r = \lim_{n \to \infty} \left| \frac{x}{n + 1} \right| = 0$, so it converges on $(-\infty, \infty)$. The first three Maclaurin polynomials are $1$, $1 + x$, and $1 + x + \frac{x^2}{2}$.

###



## The Convergence of Taylor Series

Now that we have a candidate for general series representations, the natural next thing to figure out is when they converge to the functions they're derived from. Just as we measured our approximation of series with remainders, we'll measure a Taylor series's approximation of a function with a <dfn>remainder function</dfn>. Given a Taylor series

$$
	\sum_{n = 0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n
$$

centered at $a$, let's define the $N$th remainder function as

$$
	R_N(x) = f(x) - \sum_{n = 0}^N \frac{f^{(n)}(a)}{n!} (x - a)^n.
$$

In order for the Taylor series to approximate $f$, we need $\lim_{N \to \infty} R_N(x) = 0$, and to know when that's true, we have to find a formula for $R_N(x)$. Figuring that formula out takes some effort, but the result will be simple enough in the end. A critical part will be the use of the Mean Value theorem --- let's state that now to remind ourselves how it works.



### thm "The Mean Value Theorem"
	
	Let $f$ be a function differentiable on an open interval $(a, b)$. Then there is a point $c$ with $a < c < b$ with
	
	$$
		f'(c) = \frac{f(b) - f(a)}{b - a}.
	$$

###



The idea behind the MVT is that the fraction on the right is the slope of the line between $(a, f(a))$ and $(b, f(b))$, and there is no way to draw a fucntion between those points without a tangent line somewhere having the same slope as this secant line. We'll be looking at a case where $f(a) = f(b)$, so the conclusion will be that the derivative must be zero somewhere in between.

The function we'll be applying the MVT to is somewhat complicated. Fix a value of $x \neq a$ and define a function $g$ by

$$
	g(t) = f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(t)}{n!} (x - t)^n \right) + R_N(x) \frac{(x - t)^{N + 1}}{(x - a)^{N + 1}} \right).
$$

This is a huge function, but it's not quite as complicated as it might seem. Inside the parentheses on the right is the start of a Taylor series for $f$ centered at $t$. If we added on its remainder, we'd have a complete Taylor series, but instead we add on $R_N$, which is the remainder of the series centered at $a$, and we multiply it by $\frac{(x - t)^{n + 1}}{(x - a)^{n + 1}}$, which will help us a little later.

There's unfortunately not that much motivation for why $g$ is defined this way, and the ends will have to justify the means. First of all, since $f$ is infinitely differentiable, so is $g$ --- it's just a bunch of derivatives of $f$ and polynomials in $t$. Also, we know that $g$ is zero for $t = x$ and $t = a$:

$$
	g(x) &= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(x)}{n!} (x - x)^n \right) + R_N(x) \frac{(x - x)^{N + 1}}{(x - a)^{N + 1}} \right).
	
	&= f(x) - \frac{f^{(0)}(x)}{0!}
	
	&= f(x) - f(x)
	
	&= 0.
$$

$$
	g(a) &= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) + R_N(x) \frac{(x - a)^{N + 1}}{(x - a)^{N + 1}} \right).
	
	&= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) + R_N(x) \right)
	
	&= f(x) - \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) - R_N(x)
	
	&= R_N(x) - R_N(x)
	
	&= 0.
$$

In this sense, $g$ is measuring how well a Taylor series centered at $t$ approximates $f$, except we staple on the remainder from a Taylor series centered at $a$ no matter what $t$ is.

By the Mean Value theorem, there must be a point $c$ in between $x$ and $a$ where $g'(c) = 0$. So what is $g'(c)$? Let's differentiate and find out. Remember, we're differentiating with respect to $t$, not $x$, so terms with just $x$ are treated as constants.

$$
	g'(t) &= \frac{d}{dt} \left[ f(x) - \left( f(t) + \sum_{n = 1}^N \left( \frac{f^{(n)}(t)}{n!} (x - t)^n \right) + R_N(x) \frac{(x - t)^{N + 1}}{(x - a)^{N + 1}} \right) \right]
	
	&= -f'(t) - \sum_{n = 1}^N \left( \frac{f^{(n + 1)}(t)}{n!} (x - t)^n + \frac{f^{(n)}(t)}{n!} n(x - t)^{n-1}(-1) \right) - R_N(x) \frac{(N + 1)(x - t)^N(-1)}{(x - a)^{N + 1}}
	
	&= -f'(t) - \sum_{n = 1}^N \left( \frac{f^{(n + 1)}(t)}{n!} (x - t)^n - \frac{f^{(n)}(t)}{(n-1)!} (x - t)^{n-1} \right) + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}.
$$

Incredibly, that sum telescopes! The $n = 1$ term survives on the right, and the $n = N$ survives on the left. In total, we have

$$
	g'(t) &= -f'(t) - \left( \frac{f^{(N + 1)}(t)}{N!} (x - t)^N - \frac{f'(t)}{(0)!} (x - t)^{0} \right) + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}
	
	&= -f'(t) + f'(t) - \frac{f^{(N + 1)}(t)}{N!} (x - t)^N + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}
	
	&= - \frac{f^{(N + 1)}(t)}{N!} (x - t)^N + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}.
$$

Since $g'(c) = 0$, we have that for some $c$,

$$
	0 = - \frac{f^{(N + 1)}(c)}{N!} (x - c)^N + R_N(x) \frac{(N + 1)(x - c)^N}{(x - a)^{N + 1}}.
$$

Now we can solve for $R_N(x)$ to find that

$$
	R_N(x) &= \frac{\frac{f^{(N + 1)}(c)}{N!} (x - c)^N}{\frac{(N + 1)(x - c)^N}{(x - a)^{N + 1}}}
	
	&= \frac{f^{(N + 1)}(c)}{N!} (x - c)^N \frac{(x - a)^{N + 1}}{(N + 1)(x - c)^N}
	
	&= \frac{f^{(N + 1)}(c)}{N!(N + 1)} (x - a)^{N + 1}
	
	&= \frac{f^{(N + 1)}(c)}{(N + 1)!} (x - a)^{N + 1}.
$$

Specifically, if $f^{(N + 1)}$ is a bounded function, then that bound will carry over to the remainder. This is the core of the main result about the remainders of Taylor series, which we're *finally* ready to state.



### thm "Taylor's Theorem"
	
	Let $f$ be a function that is differentiable $N + 1$ times on some interval $I$, and let $a$ be a number in $I$. Let $R_N(x)$ be the $N$th remainder function of the Taylor series for $f$ centered at $a$. Suppose further that $f^{(N + 1)}$ is bounded on $I$: in other words, there is some number $M$ with
	
	$$
		\left| f^{(N + 1)}(x) \right| \leq M
	$$
	
	for all $x$ in $I$. Then
	
	$$
		\left| R_N(x) \right| \leq \frac{M}{(N + 1)!}|x - a|^{N + 1}.
	$$
	
###



That was a lot of very dense work leading up to a pretty dense result --- let's take a second to unpack how we'll actually be using it in practice. When we have a Taylor series for a function $f(x)$, we can determine how good of an approximation the $N$th Taylor polynomial is by **effectively only looking at the next term**. For example, the quality of the approximation of a Taylor polynomial that ends at the $n = 3$ term of the Taylor series is determined only by the behavior of the $n = 4$ term. If this section's been throwing you for a bit of a loop so far, now's the time to jump back in. We'll return to our example of $\sin$ to determine the remainders for each of the Taylor polynomials.



### ex "Taylor's Theorem"
	
	Determine a bound for the $N$th remainder function of the Maclaurin series for $\sin(x)$. Using a Maclaurin polynomial of degree $5$, estimate $\sin(1)$ and determine the maximum error.
	
	First, remember that the Maclaurin series for $\sin(x)$ is
	
	$$
		\sum_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!},
	$$
	
	and in order to determine the remainder, we'll need to bound $\frac{d^{n+1}}{dx^{n+1}} [\sin(x)]$. Every derivative of $\sin(x)$ is either $\pm \sin(x)$ or $\pm \cos(x)$, and so
	
	$$
		\left| \frac{d^{n+1}}{dx^{n+1}} [\sin(x)] \right| \leq 1
	$$
	
	for all $x$. Plugging this into Taylor's remainder formula, we find that
	
	$$
		\left| R_N(x) \right| &\leq \frac{1}{(N + 1)!}|x - a|^{N + 1}
		
		&= \frac{|x|^{N + 1}}{(N + 1)!}.
	$$
	
	For example, if we'd like to know how well $p_5(x)$ approximates $\sin\left( \frac{\pi}{2} \right)$, we evaluate
	
	$$
		\left| R_5\left( \frac{\pi}{2} \right) \right| &\leq \frac{\left| \frac{\pi}{2} \right|^6}{6!}
		
		&\approx .021.
	$$
	
	Therefore, the approximation is off by at most $.021$ in either direction. This is the upper bound --- if we actually evaluate $p_5\left( \frac{\pi}{2} \right)$, we find
	
	$$
		p_5\left( \frac{\pi}{2} \right) &= \frac{\pi}{2} - \frac{\left( \frac{\pi}{2} \right)^3}{3!} + \frac{\left( \frac{\pi}{2} \right)^5}{5!}
		
		&\approx 1.0045.
	$$
	
	The actual value of $\sin\left( \frac{\pi}{2} \right)$ is exactly $1$, so in reality, $p_5\left( \frac{\pi}{2} \right)$ is off by only $.0045$.
	
	The second part of the example works similarly to this. If we want to approximate $\sin(1)$ with $p_5$, we just evaluate $p_5(1)$:
	
	$$
		p_5(1) &= 1 - \frac{1^3}{3!} + \frac{1^5}{5!}
		
		&= \frac{101}{120}
		
		&\approx .8417.
	$$
	
	By bounding $|R_5(1)|$, we can tell how good of an approximation this is.
	
	$$
		\left| R_5(1) \right| &\leq \frac{\left| 1 \right|^6}{6!}
		
		&= \frac{1}{720}
		
		&\approx .0013.
	$$
	
	This is actually how calculators and computers compute functions like $\sin(x)$, $\sqrt{x}$, and $\ln(x)$ for values of $x$ that don't have nice expressions: under the hood, it's all Taylor series.

###

### exc "Taylor's Theorem"
	
	1. With the Taylor series for $\ln(x)$ centered at $x = 1$, approximate $\ln(1.5)$ with a Taylor polynomial of degree $4$. Determine the maximum error.
	
	2. With the Taylor series for $f(x) = x^2 + 4x + 4$ centered at $x = 2$, approximate $f(0)$ with a Taylor polynomial of degree $1$ and determine the maximum error. Draw a picture of this --- what's the calculus I name for this process?
	
	3. With the Maclaurin series for $e^x$, approximate $e$ to 4 decimal places.
	
	**Solutions:**
	
	1. Using the series we computed before, the degree-4 Taylor polynomial is
	
	$$
		(x - 1) - \frac{(x - 1)^2}{2} + \frac{(x - 1)^3}{3} - \frac{(x - 1)^4}{4},
	$$
	
	so plugging in $x = 1.5$, we have
	
	$$
		.5 - \frac{(.5)^2}{2} + \frac{(.5)^3}{3} - \frac{(.5)^4}{4} \approx .401.
	$$
	
	In order to bound the remainder, we first look at $\frac{d^5}{dx^5}[\ln(x)]$, the first derviative not part of the polynomial, and try to bound it on an interval centered at $1$ and containing $1.5$. That derivative is $\frac{24}{x^5}$, and a good choice for the interval is $[.5, 1.5]$. Since $\frac{24}{x^5}$ is a decreasing function, the largest it could possibly be in absolute value on that interval is $\frac{24}{(.5)^5} = 768$. Dropping that into the remainder bound, we have
	
	$$
		|R_4(x)| \leq \frac{768}{5!}|x - 1|^5.
	$$
	
	Since $|x - 1| < .5$,
	
	$$
		|R_4(x)| \leq \frac{768}{5!}(.5)^5 = .2.
	$$
	
	Therefore, the approximation is within $.2$ of the actual value.
	
	2. This time around, we're using the polynomial $16 + 8(x - 2)$ to approximate, so we use $x = 0$ to get $16 - 16 = 0$. Now the second derivative is always $2$, so that will serve as our value of $M$. Meanwhile, the interval we should probably use is $[0, 4]$, since it's centered at $2$ and contains $0$. Therefore, we have
	
	$$
		|R_1(0)| \leq \frac{2}{2!}(2)^2 = 4.
	$$
	
	Since a degree-$1$ Taylor polynomial is just a tangent line, this is just linear approximation!
	
	3. Finally, we want to make the remainder for $e^x$ to be less than $.0001$. We have $x = 1$ and the series is centered at $0$, so a good interval to use is $[-1, 1]$. All derivatives of $e^x$ are just $e^x$, whose maximum absolute value on $[-1, 1]$ is $e$. Therefore, the remainder is bounded by
	
	$$
		|R_N(1)| \leq \frac{e}{(N + 1)!} 1^{N + 1}.
	$$
	
	We need that to be less than $.0001$ --- trying values experimentally, $N = 7$ works. Therefore, our approximation of $e$ is
	
	$$
		1 + 1 + \frac{1}{2!} + \frac{1}{3!} + \cdots + \frac{1}{7!} = 2.71825.
	$$

###



There's one final question before our basic theory of Taylor series is complete: we can find a Taylor series for a function, and we can determine the interval of convergence of that Taylor series, *and* we can bound the remainder of the series. But how can we determine if or when the Taylor series actually converges to $f$? This is where Taylor's theorem really shines: since $R_N(x)$ is the difference between the $N$th Taylor polynomial and $f$, a Taylor series converges to $f$ exactly when $\lim_{N \to \infty} R_N(X) = 0$. Once we've bounded the remainder, we can take the limit and figure out when it's zero.



### ex "convergence of a Taylor series"
	
	Determine when the Maclaurin series for $\sin(x)$ actually converges to $\sin(x)$.
	
	We know from previous examples that the Maclaurin series for $\sin(x)$ is
	
	$$
		\sum_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!},
	$$
	
	and that the $N$th remainder $R_N(x)$ is bounded by
	
	$$
		\left| R_N(x) \right| \leq \frac{|x|^{N + 1}}{(N + 1)!}.
	$$
	
	Now we can take the limit as $N \to \infty$:
	
	$$
		\lim_{N \to \infty} \left| R_N(x) \right| &\leq \lim_{N \to \infty} \frac{|x|^{N + 1}}{(N + 1)!}
		
		&= 0.
	$$
	
	To see why the limit is zero, remember that $x$ is fixed. No matter how big $|x|$ is, for large enough $N$ the denominator will be multiplying by massive integers, while the numerator is multiplying only by $|x|$. In other words, a factorial grows faster than any exponential.
	
	Since the limit is zero no matter what $x$ is, the series converges to $\sin(x)$ *everywhere*. This doesn't mean that we should always use a series centered at $x = 0$ to approximate any value of $\sin$, though --- as we've seen from the graph, Taylor polynomials are better approximations closer to where they're centered, so we should always take the cloest center we can (i.e. a center where we can easily calculate all the derivatives). For example, if we're trying to approximate $\sin(10)$, a good center would be $x = 3\pi \approx 9.425$, since we know the values of $\sin$ and all its derivatives at $3\pi$. Let's briefly draw a graph to illustrate this.
	
	### desmos taylorSeriesComparison
	
	Here, the blue and red functions are degree-5 Taylor polynomials for $\sin$ centered at $0$ and $3\pi$, respectively. The actual value of $\sin(10)$ is approximately $-.544021$, but while the red function predicts a value of $-.544025$, the blue function has stopped tracking with $\sin(x)$ long before $x = 10$, and it predicts a garbage value of $676.7$. We can increase the degree of the polynomials to help fix this problem, but there's often no substitute for centering them in the right place to begin with!

###



As one final note, Taylor series aren't guaranteed to converge to the function they were derived from for any $x$-value other than their center. Let's take a look at an example of this, where the process of generating the Taylor series goes catastrophically wrong.

Consider the function

$$
	f(x) = \begin{cases} e^{-\frac{1}{x^2}}, & x \neq 0 \\ 0, & x = 0 \end{cases}
$$

This function is differentiable, but it is *extremely* flat at $x = 0$ --- so flat that every single derivative is zero.

### desmos problematicTaylorSeries

The Maclaurin series for $f$ is then just

$$
	0 + \frac{0}{1!}x + \frac{0}{2!}x^2 + \frac{0}{3!}x^3 + \cdots = 0.
$$

Therefore, $f$ is equal to its Maclaurin series *only* at $x = 0$.



### as "analytic functions"

	A function $f$ whose Taylor series centered at $x = a$ converges to $f$ in some interval around $a$ is called **analytic** at $a$. Functions that are differentiable infinitely many times at $x = a$ are called **smooth** at $a$, and since constructing a Taylor series requires being able to take infinitely many derivatives, all analytic functions are necessarily smooth. These conditions slot in at the top end of differentiability conditions, and they all have convenient notations: if a function is continuous, we say it's $C^0$, if it's differentiable with a continuous derivative, we say it's $C^1$, if it's twice-differentiable with a continuous second derivative, we say it's $C^2$, and so on. If a function is smooth, it's $C^\infty$, and if it's analytic, it's $C^\omega$ (here, $\omega$ denotes the idea of the first counting number *after* all of the positive integers).
	
###

We're almost done with the course! The final section will see us applying Taylor series to all sorts of areas of math to great effect, making good use of all the work we've done throughout the class.



### nav-buttons