### nav-buttons



So far in the course, we've covered many of the most important techniques for solving DEs. There are a lot more we could discuss, but just like with integration techniques, they become less and less generally applicable the farther we go, both among DEs in general and among those that actually occur in science. Instead, we'll turn to another type of problem that's both solvable and extremely common: systems of DEs. Here's the one-sentence pitch: most physical systems whose rates of change we can express with a differential equation depend not only on their own state and derivatives, but on other systems' states and derivatives too. One of the most famous examples is a **predator-prey system**: if $x(t)$ gives the population of a prey animal at time $t$ and $y(t)$ gives the population of a predator animal, then $x'$ and $y'$ will both depend on $x$ and $y$, since the population growth rate of the prey depends both on its current population and the population of the predators. To write down some concrete equations, let's look at the **Lotka-Volterra model**, which aims to model such a system:

$$
	x' &= ax - bxy
	
	y' &= -cy + dxy,
$$

where $a$, $b$, $c$, and $d$ are constants that describe the particular conditions of the system. Breaking this apart, it makes quite a bit of sense: prey animals (e.g. rabbits) typically grow exponentially in number (ignoring the carrying capacity of the environment itself), so for a variable $a$ determining the growth rate, we'd expect $x' = ax$. That's not the only thing influencing the rabbit population, though --- the more of the predator (e.g. foxes) are present, *and* the more rabbits there are, the more will be eaten: hence the $bxy$ term, which increases with both $x$ and $y$. Foxes' and other predators' populations typically do *not* grow exponentially. When food is scarce or nonexistent, they decay exponentially instead --- and this is reflected by the $-cy$ term. The fox population increases when both it and the rabbit population are high, and this is the point of the $dxy$ term.

The Lotka-Volterra model does a surprisingly great job of modeling real-life predator-prey systems while being simple enough to be solvable. We currently don't have a single technique to help us solve it, but in just a few sections, we'll be able to return to systems like this one and solve them without too much trouble.

The tools we'll need to solve systems of DEs reach far beyond the subject of differential equations, and we'll take this section to get acquainted with many of the key ideas before returning to DEs in the next.



## Vectors and Matrices

Let's return to the world of systems of algebraic equations. A system of two equations and two unknowns, say $x_1$ and $x_2$, might look something like this:

$$
	2x_1 - x_2 &= 3
	
	x_1 + 5x_2 &= -1.
$$

The details that distinguish this system from any other aren't the $x_1$ and $x_2$, but the *coefficients* --- the six numbers specify the exact system we're dealing with. Let's focus on the left sides in particular. By forgetting the $x_i$, we get a $2 \times 2$ grid of numbers:

$$
	\begin{array}{cc} 2 & -1 \\ 1 & 5 \end{array}
$$

The big leap is to treat these four numbers as one large object. To indicate the distinction, we'll wrap it in brackets (parentheses are also commonly used), and to remind ourselves that it's a somewhat more complex variable than a single number, we'll represent it with a capital --- and in printed text, bold --- letter.

$$
	**A** = [[ 2, -1 ; 1, 5 ]].
$$

We call $**A**$ a $2 \times 2$ **matrix** --- other sizes, including non-square ones, are also possible, and we'll look at examples momentarily.

In the same way that an ordered pair $(x, y)$ has two degrees of freedom --- both $x$ and $y$ can be changed independently --- $**A**$ has four. To reference the individual elements of $**A**$, we typically use the corresponding lowercase letter with indices. For example, the elements of $**A**$ are $a_{11}$, $a_{12}$, $a_{21}$, and $a_{22}$. The first number indexes the row and the second the column, so

$$
	a_{11} &= 2
	
	a_{12} &= -1
	
	a_{21} &= 1
	
	a_{22} &= 5.
$$

You might find the idea of matrices fascinating in its own right, or you might be waiting for the applications --- either way, we need to actually use these things for something to justify the extra notation and definitions. Getting back to our systems of equations, we want to be able to represent both left sides of the equations at once with some sort of expression involving $**A**$. A clue toward the answer lies in the right side of the equation. This time, there aren't any $x_i$ to drop, and we can just assemble the numbers into a matrix directly.

$$
	**b** = [[ 3 ; -1 ]].
$$

When a matrix only has a single column, we call it a **column vector** and denote it with a bold but lowercase letter. In handwriting, we'll typically denote vectors with a little arrow above them, as in $\vec{b}$.

In order for two vectors to be equal, all of their entries must be the same, and this indicates the direction we should go: we can represent the two equations in the system as a single equation involving vectors:

$$
	[[ 2x_1 - x_2 ; x_1 + 5x_2 ]] = **b**.
$$

On the surface, we haven't changed much of anything, but we're actually getting close to something profound. Since $**A**$ is providing the coefficients for all the $x_i$, we should be able to just have $**A**$ *be a coefficient itself*. To that end, let's define a vector $**x**$ that stores both of our variables:

$$
	**x** = [[ x_1 ; x_2 ]].
$$

Now the final part: if $**A**$ is going be a single coefficient, we want to be able to write $**Ax**$, multiplying a matrix and a column vector, to mean

$$
	**Ax** = [[ 2, -1 ; 1, 5 ]] [[ x_1 ; x_2 ]] = [[ 2x_1 - x_2 ; x_1 + 5x_2 ]].
$$

So, to bring it all together: for matrices to actually represent coefficients in the way we want them to, this is how matrix-vector multiplication should be defined. Let's write this out in a little more generality.



### def "$2 \times 2$ matrix-vector multiplication"
	
	Let $**A**$ and $**x**$ be a matrix and vector, respectively, defined as
	
	$$
		**A** &= [[ a, b ; c, d ]]
		**x** &= [[ x_1 ; x_2 ]].
	$$
	
	Then the product $**Ax**$ is a column vector, defined to be
	
	$$
		**Ax** = [[ a, b ; c, d ]] [[ x_1 ; x_2 ]] = [[ ax_1 + bx_2 ; cx_1 + dx_2 ]].
	$$
	
	One way to visualize this is by taking the vector $**x**$, rotating it sideways so that it reads $[[ x_1, x_2 ]]$, and then for each row of $**A**$, multiplying the two together and adding the results.
	
###



As we'll see in the next few examples, matrix-vector multiplication is defined for larger than $2 \times 2$ matrices: for the definition to work, the only restriction is that the length of $**x**$ is equal to the row length of $**A**$ --- otherwise there'd be a mismatch we turn $**x**$ sideways.



### ex "matrix-vector multiplication"
	
	Evaluate the following products:
	
	1. $$[[ 1, -1 ; 2, 3 ]] [[ 3 ; -2 ]].$$
	
	Turning the vector sideways and multiplying through, we have
	
	$$
		[[ 1(3) + -1(-2) ; 2(3) + 3(-2) ]] = [[ 5 ; 0 ]].
	$$
	
	2. $$[[ 5, 0, 2 ; 2, 3, 0 ]] [[ 1 ; 0 ; -2 ]].$$
	
	Although the matrix isn't square and larger than $2 \times 2$, the definition still works fine: we just turn the vector sideways and multiply it with every row.
	
	$$
		[[ 5(1) + 0(0) + 2(-2) ; 2(1) + 3(0) + 0(-2) ]] = [[ 1 ; 2 ]].
	$$
	
	3. $$[[ 1, 0, 0 ; 0, 1, 0 ; 0, 0, 1 ]] [[ 5 ; -4 ; 1 ]].$$
	
	This one works exactly the same way! The resulting vector is
	
	$$
		[[ 1(5) + 0(-4) + 0(1) ; 0(5) + 1(-4) + 0(1) ; 0(5) + 0(-4) + 1(1) ]] = [[ 5 ; -4 ; 1 ]].
	$$
	
###

### exc "matrix-vector multiplication"
	
	Evaluate the following products:
	
	1. $$[[ 1, 0 ; 0, 1 ]] [[ 5 ; 7 ]].$$
	
	2. $$[[ 1, 1 ; 2, 1 ; -1, 1 ]] [[ -2 ; 5 ]].$$
	
	3. $$[[ 1, 1 ; 2, 1 ; -1, 1 ]] [[ -3 ; 6 ; 1 ]].$$
	
###



From these examples, we can see some general properties of matrix-vector multiplication. If $**A**$ is an $n \times k$ matrix, meaning it has $n$ rows and $k$ columns, then $**Ax**$ is defined only if $**x**$ has length $k$, and the length of $**Ax**$ is $n$.

A square matrix with all $0$s with a strip of $1$s down the diagonal from top-left to bottom-right, like those in part 3 of the example or part 1 of the exercise, seem to multiply vectors by not changing them, and it's not too hard to see that the same will happen with every vector: the $i$th row pulls out exactly the $i$th component. This matrix is useful enough that it deserves its own name.



### def "identity matrix"
	
	The $n \times n$ **identity matrix** consists of all $0$s, except for $1$s down the top-left to bottom-right diagonal (called the **main diagonal**). We write it as $**I**_n$, or just $**I**$ if the dimension is clear from context.
	
###



Now that we're used to the basic idea of matrices, let's look at some of their basic properties. The guiding principle to keep in mind is that **matrices are functions** that operate by multiplying: in other words, every matrix $**A**$ corresponds to a linear function $T$ so that $**Ax** = T(**x**)$. To that end, we want to define matrix addition so that it acts just like function addition. In other words, matrices $**A**$ and $**B**$ should have a sum so that

$$
	(**A** + **B**)**x** = **Ax** + **Bx**,
$$

just like functions. Therefore, we need $**A**$ and $**B**$ to have the same number of columns so they can both multiply $**x**$, and the same number of rows so that they can add together after multiplying. For vector multiplication to distribute like this, we just need the $i$th row of $**A** + **B**$ to be the $i$th row of $**A**$ plus the $i$th row of $**B**$. All of this is to say that matrix addition works exactly like we'd hope: as long as they're exactly the same shape, we can just add each component together. For example,

$$
	[[ 5, 0, 2 ; 2, 3, 0 ]] + [[ -1, 2, 0 ; -5, 0, 2 ]] &= [[ 5 - 1, 0 + 2, 2 + 0 ; 2 - 5, 3 + 0, 0 + 2 ]]
	
	&= [[ 4, 2, 2 ; -3, 3, 2 ]].
$$

For exactly the same reason, scalar multiplication works the same way: for a number $c$, the matrix $c**A**$ is created by multiplying every entry by $c$. For example,

$$
	-4[[ 5, 0, 2 ; 2, 3, 0 ]] = [[ -20, 0, -8 ; -8, -12, 0 ]].
$$



## Matrix Multiplication

This one topic has a hefty amount of explanation behind it. Far, far too often, matrix multiplication is introduced as a quirky, complicated operation and then never justified or dug into. It'll take us a minute to go through the details, but we're going to make sure we understand why matrices multiply the way they do.

To really hammer home the point, matrices are functions. An $n \times k$ matrix $**B**$ takes in a $k$-dimensional vector $**x**$ and gives back an $n$-dimensional one by doing matrix-vector multiplication, which we write $**Bx**$. If we have another matrix $**A**$ and we want the product $**AB**$ to make any sense at all, then when we multiply by $**x**$, the following equality should hold:

$$
	(**AB**)**x** = **A**(**Bx**).
$$

On the left is a matrix called $**AB**$ that we don't know how to calculate yet, but on the right, there's just matrix-vector multiplication happening twice. Since we're thinking of matrices as being functions that act by multiplication, the right side is function composition, so, crucially, the left side should be too. In other words, **matrix multiplication should be defined to be composition of functions**. This point is glossed over or ignored in many introductory classes, and that's really a shame! Understanding why matrices work they way they do is half the battle in using them properly.

So --- how do we do that? First of all, not any two matrices can be multiplied together. Since an $n \times k$ matrix $**B**$ takes in a $k$-dimensional vector $**x**$ and gives back an $n$-dimensional one $**Bx**$, a matrix $**A**$ that makes $**AB**$ make sense needs to take in $n$-dimensional vectors. In other words, $**A**$ needs to be an $m \times n$ matrix for some $n$. Since $**ABx** = (**A**)**Bx**$, the matrix $**AB**$ should be $m \times k$.

To figure out the exact formula for matrix multiplication, let's look at a few vectors in particular. Let $**e_j**$ be a $k$-dimensional column vector defined by

$$
	**e_j** = [[ 0 ; \vdots ; 0 ; 1 ; 0 ; \vdots ; 0 ]],
$$

where the $1$ is in position $j$. When we evaluate $**Be_j**$ by turning $**e_j**$ sideways, only column $j$ of $**B**$ is picked out --- in other words, if we write

$$
	**B** = [[ **\mid**, **\mid**,, **\mid**, **\mid** ; **b_1**, **b_2**, \cdots, **b_{k - 1}**, **b_k** ; **\mid**, **\mid**, , **\mid**, **\mid** ]],
$$

so that $**b_j**$ is the $j$th column of $**B**$, then $**Be_j** = **b_j**$. By the same token, $(**AB**)**e_j** = **Ab_j**$ is the $j$th column of $**AB**$. So the matrix product $**AB**$ is defined by

$$
	**AB** = [[ **\mid**, **\mid**,, **\mid**, **\mid** ; **Ab_1**, **Ab_2**, \cdots, **Ab_{k - 1}**, **Ab_k** ; **\mid**, **\mid**, , **\mid**, **\mid** ]].
$$

Thankfully, we can streamline this formula a little to make it easier to remember. To get the $i$th entry of $**Ab_j**$, we turn $**b_j**$ sideways and look at its product with the $i$th row of $**A**$. So we can get a formula for $**AB**$ one entry at a time.



### thm "matrix multiplication"
	
	Let $**A**$ be an $m \times n$ matrix and let $**B**$ be an $n \times k$ one. Then $**AB**$ is an $m \times k$ matrix, and the entry in row $i$ and column $j$ is defined by
	
	$$
		**AB**_{ij} = \sum_{l = 1}^n a_{il}b_{lj}.
	$$
	
	In other words, we take row $i$ from $**A**$ and column $j$ from $**B**$, multiply the corresponding entries, and add all $n$ together. When thinking of a matrix as a function operating on vectors, matrix multiplication *is* function composition. In other words,
	
	$$
		(**AB**)**x** = **ABx** = **A**(**Bx**).
	$$
	
###



That was a whole lot of work! Let's turn to some examples to see this in action.



### ex "matrix multiplication"
	
	Let $$**A** = [[ 5, 0 ; -2, 1 ]]$$ and $$**B** = [[ 3, 1, -1 ; -1, 0, 2 ]]$$. Evaluate $**AB**$.
	
	This is a valid product to take, since the number of columns $**A**$ and the number of rows of $**B**$ are the same ($2$). And the dimensions of $**AB**$ will be $2 \times 3$, since $**A**$ has $2$ rows and $**B**$ has $3$ columns. To get the element $**AB**_{11}$ --- the top-left entry --- we take the first row of $**A**$, which is $(5, 0)$, and the first column of $**B**$, which is $(3, -1)$, and multiply them together to get $5(3) + 0(-1) = 15$. Repeating this for the other five entries, we get
	
	$$
		**AB** = [[ 15, 5, -5 ; -7, -2, 4 ]].
	$$
	
###

### exc "matrix multiplication"
	
	Let $**x** = [[ 1 ; 2 ; -1 ]]$. With the same matrices from the previous example, evaluate $(**AB**)**x**$ and $**A**(**Bx**)$ and show they're the same.
	
###



Let's mention another critical aspect of matrices: multiplication isn't commutative, meaning $**AB** \neq **BA**$. In fact, even if we know $**AB**$, it's not guaranteed that $**BA**$ is even defined. The matrices $**A**$ and $**B**$ from the previous example can't be multiplied in the other order. Just like two functions $f$ and $g$ almost never satisfy $f(g(x)) = g(f(x))$, matrices almost never commute.

In the next section, we'll begin putting matrices to use --- our first application will be solving large systems of equations incredibly efficiently.



### nav-buttons