### nav-buttons



After three sections of linear algebra, let's parlay our knowledge of matrices into differential equations. As mentioned before, we'll be looking at *systems* of DEs now, and that hopefully isn't too surprising, given how we've been using matrices to solve systems of algebraic equations.

Let's start small, with first-order linear DEs. If we have $n$ variables $x_1, ..., x_n$ which are all functions of $t$, then in general, every $x_i$ can be a function of all $n$ of the other $x_j$, as well as a function of $t$ itself. In symbols, there are functions $p_{ij}$ with

$$
	x_1' &= p_{11}(t)x_1 + p_{12}(t)x_2 + \cdots + p_{1n}(t)x_n + g_1(t)
	
	x_2' &= p_{21}(t)x_1 + p_{22}(t)x_2 + \cdots + p_{2n}(t)x_n + g_2(t)
	
	& \ \ \vdots
	
	x_n' &= p_{n1}(t)x_1 + p_{n2}(t)x_2 + \cdots + p_{nn}(t)x_n + g_n(t).
$$

This couldn't look much more like a matrix! If we define a vector $**x**$ as

$$
	**x**(t) = [[ x_1(t) ; \vdots ; x_n(t) ]],
$$

and define differentiation to be componentwise --- i.e.

$$
	**x**' = [[ x_1' ; \vdots ; x_n' ]],
$$

then we're most of the way there. Let's define an $n \times n$ matrix $**P**$ and a vector $**g**$ of length $n$ as

$$
	**P**(t) &= [[ p_{11}(t), \cdots, p_{1n}(t) ; \vdots, \ddots, \vdots ; p_{n1}(t), \cdots, p_{nn}(t) ]]
	
	**g**(t) &= [[ g_1(t) ; \vdots ; g_n(t) ]]
$$

Now we can restate the $n$ interdependent DEs as simply

$$
	**x**' = **Px** + **g**.
$$

The story of systems of DEs parallels the story of individual ones incredibly closely, and we'll want to start by looking at **homogeneous** systems: the ones where $**g** = **0**$. For exactly the same reason as with individual linear homogeneous DEs, if two solutions are $**x_1**$ and $**x_2**$, then any linear combination $c_1**x_1** + c_2**x_2**$ will be a solution. In general, a system of $n$ first-order linear DEs will have $n$ fundamental (i.e. linearly independent) solutions, so the general solution will be

$$
	**x** = c_1 **x_1** + \cdots + c_n **x_n**.
$$

Let's take a moment to notice the notation conflict here: in this equation, $**x_1**$ is an entire length-$n$ vector, as opposed to the first entry of a vector $**x**$, which is written $x_1$. Thankfully, the situations where we need to distinguish between the two notations will be few and far between.

We didn't talk about the Wronskian for first-order linear DEs because it's not very interesting: an order-$1$ DE has a Wronskian that's just the determinant of a $1 \times 1$ matrix --- i.e. its single entry. On the other hand, the Wronskian of a system of $n$ DEs is quite a bit more substantial. Picking through the details is a little bit tedious, but the end result is similar to an $n$th order DE: for fundamental solutions $**x_1**, ..., **x_n**$, the Wronskian is

$$
	W[**x_1**, ..., **x_n**] = \det [[ \mid, , \mid ; **x_1**, \cdots, **x_n** ; \mid, , \mid ]]
$$

In other words, we just put the $**x_i**$ in as column vectors and take the determinant. The result is a familiar one: if the Wronskian is ever nonzero, then we've successfully found the general solution, and if it's nonzero when we plug in $t = t_0$, then the initial condition $**x**(t_0) = **x_0**$ is solvable. (As an aside, both of those statements should make a lot more sense now that we've seen what determinants tell us!)



## Solving First-Order Systems

Enough abstract work --- let's get to solving some systems! As usual, we'll start small and work our way to general results.



### ex "a first-order system"
	
	Find the general solution to
	
	$$
		x_1' &= 5x_1
		
		x_2' &= -7x_2.
	$$
	
	The matrix form of the DE is
	
	$$
		**x**' = [[ 5, 0 ; 0, -7 ]] **x**,
	$$
	
	but since the two DEs are completely independent, this barely qualifies as a system. We can just solve them separately (with integrating factors, separation, or even a characteristic equation) and then tack the solutions together:
	
	$$
		x_1 &= c_1e^{5t}
		
		x_2 &= c_2e^{-7t}
	$$
	
	Our fundamental solutions need to be vectors, so there's a slight modification needed here: the first one doesn't depend on $x_2$ and the second doesn't depend on $x_1$, so our final answer is
	
	$$
		**x** = c_1 [[ e^{5t} ; 0 ]] + c_2 [[ 0 ; e^{-7t} ]].
	$$
	
###



That might seem like a poor choice for a first example --- there wasn't anything substantively different than solving two first-order equations separately. But it actually will motivate quite a bit of the more complicated systems to come by telling us that exponential functions are a decent guess. We probably won't have the luxury of having half of the vector be zero, but let's see what information we can extract with a more general guess. For a general system $**x**' = **Ax**$, let's guess that a solution is $**x** = e^{\lambda t}**v**$ for some value $\lambda$ and vector $**v**$. In our previous example, the first solution had $\lambda = 5$ and

$$
	**v** = [[ 1 ; 0 ]]
$$

If we assume $**x**$ is a solution, then

$$
	**x**' &= **Ax**
	
	\lambda e^{\lambda t}**v** &= e^{\lambda t} **Av**
	
	\lambda **v** &= **Av**,
$$

and in a single equation, we've justified the entire previous section: we need to find the eigenvalues and eigenvalues! Specifically,



### thm "solving first-order linear homogeneous systems"
	
	Let $**x**' = **Ax**$ be a first-order linear homogeneous system of DEs, where $**A**$ is a real-valued matrix. If $**v**$ is an eigenvalue of $**A**$ with eigenvalue $\lambda$, $**x** = e^{\lambda t} **v**$ is a solution to the system.
	
###



If all $n$ eigenvalues of $**A**$ are real and different, then it's guaranteed that the corresponding $n$ eigenvectors are different too (if two were the same, they'd have to have the same eigenvalue). If some eigenvalues are complex, we'll have to mess with complex exponentials a bit. Since they're roots of a polynomial with real coefficients, though, they'll have to occur in conjugate pairs, so it won't be too bad. Finally, if some eigenvalues are repeated, then there may not be $n$ linearly independent eigenvectors, and we'll have to apply something like reduction of order. Hopefully this is all sounding familiar! The story of $n$th-*order* individual DEs and that of $n \times n$ first-order *systems* play out in much the same way. We'll deal with distinct real eigenvalues first, and complex and repeated eigenvalues in the next section.



### ex "a first-order system"
	
	Solve 
	
	$$
		x_1' &= x_1 + 3x_2
		
		x_2' &= 4x_1 + 2x_2.
	$$
	
	and verify that you've found the general solution with the Wronskian.
	
	First of all, the matrix form is
	
	$$
		**x**' = [[ 1, 3 ; 4, 2 ]] **x**,
	$$
	
	so we need to find the eigenvalues and eigenvectors of that matrix. Subtracting $\lambda$ from the diagonal and taking the determinant gives us
	
	$$
		\det [[ 1 - \lambda, 3 ; 4, 2 - \lambda ]] &= 0
		
		(1 - \lambda)(2 - \lambda) - (3)(4) &= 0
		
		2 - 3\lambda + \lambda^2 - 12 &= 0
		
		\lambda^2 - 3\lambda - 10 &= 0
		
		(\lambda - 5)(\lambda + 2) &= 0
		
		\lambda = 5, \quad \lambda &= -2.
	$$
	
	Now we can find the eigenvectors by plugging these values of $\lambda$ back in:
	
	$$
		[[ 1 - 5, 3 ; 4, 2 - 5 ]] **v** &= **0**
		
		[[ -4, 3 | 0 ; 4, -3 | 0 ]] &
		
		[[ -4, 3 | 0 ; 0, 0 | 0 ]] & \qquad **r_2** \pe -1**r_1**
		
		-4v_1 + 3v_2 &= 0.
	$$
	
	Therefore, $v_1 = \frac{3}{4}v_2$, so to keep the numbers nice, let's take $v_2 = 4$ and $v_1 = 3$ to get out first fundamental solution of
	
	$$
		**x_1** = e^{5t} [[ 3 ; 4 ]]
	$$
	
	Now we'll repeat the process with the other eigenvalue:
	
	$$
		[[ 1 + 2, 3 ; 4, 2 + 2 ]] **v** &= **0**
		
		[[ 3, 3 | 0 ; 4, 4 | 0 ]] &
		
		[[ 1, 1 | 0 ; 4, 4 | 0 ]] & \qquad **r_1** \te \frac{1}{3}
		
		[[ 1, 1 | 0 ; 0, 0 | 0 ]] & \qquad **r_2** \pe -4**r_1**
		
		v_1 + v_2 &= 0.
	$$
	
	This time around, $v_1 = -v_2$, so a good choice is $v_2 = 1$ and $v_1 = -1$, making our second fundamental solution
	
	$$
		**x_2** = e^{-2t} [[ -1 ; 1 ]].
	$$
	
	Our general solution is therefore
	
	$$
		**x** &= c_1**x_1** + c_2**x_2**
		
		&= c_1 e^{5t} [[ 3 ; 4 ]] + c_2 e^{-2t} [[ -1 ; 1 ]].
	$$
	
	To verify this with the Wronskian, we just put $**x_1**$ and $**x_2**$ as column vectors in a matrix and take the determinant:
	
	$$
		W[**x_1**, **x_2**] &= \det [[ 3e^{5t}, -e^{-2t} ; 4e^{5t}, e^{-2t} ]]
		
		&= 3e^{3t} + 4e^{3t}
		
		&= 7e^{3t}.
	$$
	
	That's never zero, so any initial condition will have a solution, and it's not constantly equal to zero, meaning we successfully found the general solution.
	
	This example's been going for a while, but there's one more thing to discuss: now that we're back with first-order DEs, we can talk about direction fields again! Previously, $y'$ was a function of $y$ and $t$, but $t'$ was just equal to $1$, so although $y$ could move around, $t$ advanced at a constant rate. In the direction field, this meant we only plotted slopes, and in the particle flow visualization, it meant everything was always flowing to the right. Here, things are different: we'll need to plot $x_1$ and $x_2$ on the $x$- and $y$-axes, and they're *both* changing over time, so the fields will be much more complex. Instead of direction fields, these are called **vector fields**, since every point gets an entire vector associated with it.
	
	### desmos vectorField
	
	It's critical to emphasize that although this looks like a direction field, **neither axis is time**. Both $x_1$ and $x_2$ change over time, but the important thing is that they change based on their own values and each other's values. Curves flowing through this field take a pair of states $x_1$ and $x_2$ independent of $t$ and show how $x_1$ and $x_2$ will change over time. For example, if $x_1 = 0$ and $x_2 = 2$, then both $x_1$ and $x_2$ will eventually grow without bound --- the point gets swept up and to the right. Just like with direction fields, we can visualize the entire field as a flow of thousands of particles, and with more directions available, it's even more beautiful than before.
	
	### canvas vector-field
	
###

### exc "a first-order system"
	
	Solve the system
	
	$$
		x_1' &= 5x_1 + 2x_2
		
		x_2' &= 4x_1 + 7x_2.
	$$
	
	Verify that you've found the general solution with the Wronskian, and then sketch a direction field.
	
###



## Applications

Systems of first-order DEs crop up all the time in physical applications --- for our first, let's look at a variation on the tank problems from section 2.



### ex "multiple tanks"
	
	Three tanks are set up next to one another. Tank 1 initially contains $100\,L$ of water in which $990\,g$ of salt is dissolved, tank 2 initially contains $1\,L$ of water in which $500\,g$ of salt is dissolved, and tank 3 initially contains $50\,L$ of water. At time $t = 0$, we open a lot of valves: a pure water solution is poured into tank 1 at a rate of $2\,\frac{L}{s}$, and the well-mixed solution from tank 1 is drained into tank 2 at the same rate. Similarly, the mixed solution in tank 2 drains into tank 3, and the mixed solution in tank 3 drains and is discarded, all at $2\,\frac{L}{s}$. Find the mass of salt in each tank after 5 minutes.
	
	There's a lot to do here! First of all, let's define some variables. Let $x_1$, $x_2$, and $x_3$ be the *masses* of salt in tanks 1, 2, and 3 at time $t$. Then the derivative of each is the inflow rate minus the outflow rate to that particular tank.
	
	$$
		x_1' &= -2\frac{x_1}{100} = -\frac{1}{50}x_1
		
		x_2' &= 2\frac{x_1}{100} - 2\frac{x_2}{1} = \frac{1}{50}x_1 - 2x_2
		
		x_3' &= 2\frac{x_2}{1} - 2\frac{x_3}{50} = 2x_2 - \frac{1}{25}x_3.
	$$
	
	In total, our matrix equation is
	
	$$
		**x**' = [[ -\frac{1}{50}, 0, 0 ; \frac{1}{50}, -2, 0 ; 0, 2, -\frac{1}{25} ]]**x**.
	$$
	
	Solving for the eigenvalues, we have
	
	$$
		\det [[ -\frac{1}{50} - \lambda, 0, 0 ; \frac{1}{50}, -2 - \lambda, 0 ; 0, 2, -\frac{1}{25} - \lambda ]] = 0.
	$$
	
	To take this determinant, let's expand along the first row.
	
	$$
		\left(-\frac{1}{50} - \lambda\right) \left( \left(-2 - \lambda\right)\left(-\frac{1}{25} - \lambda\right) - 0 \right) &= 0
		
		\lambda = -\frac{1}{50}, \quad \lambda = -2, \quad \lambda &= -\frac{1}{25}.
	$$
	
	Now each of these gets plugged back in to solve for the corresponding eigenvector.
	
	$$
		[[ -\frac{1}{50} + \frac{1}{50}, 0, 0 | 0 ; \frac{1}{50}, -2 + \frac{1}{50}, 0 | 0 ; 0, 2, -\frac{1}{25} + \frac{1}{50} | 0 ]] & 
		
		[[ 0, 0, 0 | 0 ; \frac{1}{50}, \frac{-99}{50}, 0 | 0 ; 0, 2, -\frac{1}{50} | 0 ]] & 
		
		[[ 0, 0, 0 | 0 ; 1, -99, 0 | 0 ; 0, -100, 1 | 0 ]] & \qquad \begin{array}{l} **r_2** \te 50 \\ **r_3** \te -50 \end{array}
		
		v_1 = 99v_2, & \quad v_3 = 100v_2.
	$$
	
	Taking $v_2 = 1$, our first eigenvector is
	
	$$
		**v** = [[ 99 ; 1 ; 100 ]].
	$$
	
	Now we just need to repeat this for the other eigenvalues. When $\lambda = 2$, we have
	
	$$
		[[ -\frac{1}{50} + 2, 0, 0 | 0 ; \frac{1}{50}, -2 + 2, 0 | 0 ; 0, 2, -\frac{1}{25} + 2 | 0 ]] & 
		
		[[ \frac{99}{50}, 0, 0 | 0 ; \frac{1}{50}, 0, 0 | 0 ; 0, 2, \frac{49}{25} | 0 ]] & 
		
		[[ 1, 0, 0 | 0 ; 1, 0, 0 | 0 ; 0, 50, 49 | 0 ]] & \qquad \begin{array}{l} **r_1** \te \frac{50}{99} \\ **r_2** \te 50 \\ **r_3** \te 25 \end{array}
		
		[[ 1, 0, 0 | 0 ; 0, 0, 0 | 0 ; 0, 50, 49 | 0 ]] & \qquad **r_2** \pe -**r_1**
		
		v_1 = 0, & \quad v_2 = -\frac{49}{50} v_3.
	$$
	
	With $v_3 = 50$, we have an eigenvector of
	
	$$
		**v** = [[ 0 ; -49 ; 50 ]].
	$$
	
	Finally, we have $\lambda = \frac{1}{25}$:
	
	$$
		[[ -\frac{1}{50} + \frac{1}{25}, 0, 0 | 0 ; \frac{1}{50}, -2 + \frac{1}{25}, 0 | 0 ; 0, 2, -\frac{1}{25} + \frac{1}{25} | 0 ]] & 
		
		[[ \frac{1}{50}, 0, 0 | 0 ; \frac{1}{50}, -\frac{49}{25}, 0 | 0 ; 0, 2, 0 | 0 ]] & 
		
		[[ 1, 0, 0 | 0 ; 1, -98, 0 | 0 ; 0, 1, 0 | 0 ]] & \qquad \begin{array}{l} **r_1** \te 50 \\ **r_2** \te 50 \\ **r_3** \te \frac{1}{2} \end{array}
		
		[[ 1, 0, 0 | 0 ; 0, 0, 0 | 0 ; 0, 1, 0 | 0 ]] & \qquad \begin{array}{l} **r_2** \pe -**r_1** \\ **r_2** \pe 98**r_3** \end{array}
		
		v_1 = 0, & \quad v_2 = 0.
	$$
	
	That leaves $v_3$ undetermined, so let's just set $v_3 = 1$. Then our eigenvector is
	
	$$
		**v** = [[ 0 ; 0 ; 1 ]].
	$$
	
	At long last, we're done! Our general solution is
	
	$$
		**x** = c_1 e^{-t / 50} [[ 99 ; 1 ; 100 ]] + c_2 e^{-2t} [[ 0 ; -49 ; 50 ]] + c_3 e^{-t / 25} [[ 0 ; 0 ; 1 ]].
	$$
	
	Now we just need to find our particular solution:
	
	$$
		c_1 e^{0} [[ 99 ; 1 ; 100 ]] + c_2 e^{0} [[ 0 ; -49 ; 50 ]] + c_3 e^{0} [[ 0 ; 0 ; 1 ]] &= [[ 990 ; 500 ; 0 ]]
		
		[[ 99, 0, 0 | 990 ; 1, -49, 0 | 500 ; 100, 50, 1 | 0 ]] & 
		
		[[ 1, 0, 0 | 10 ; 1, -49, 0 | 500 ; 100, 50, 1 | 0 ]] & \qquad **r_1** \te \frac{1}{99}
		
		[[ 1, 0, 0 | 10 ; 0, -49, 0 | 490 ; 100, 50, 1 | 0 ]] & \qquad **r_2** \pe -**r_1**
		
		[[ 1, 0, 0 | 10 ; 0, 1, 0 | -10 ; 100, 50, 1 | 0 ]] & \qquad **r_2** \te -\frac{1}{49}
		
		[[ 1, 0, 0 | 10 ; 0, 1, 0 | -10 ; 0, 0, 1 | -500 ]] & \qquad \begin{array}{l} **r_3** \pe -100**r_1** \\ **r_3** \pe -50**r_2** \end{array}
		
		c_1 = 10, \quad c_2 = -10, & \quad c_3 = -500.
	$$
	
	The original problem asked for the masses of salt after 5 minutes, which is when $t = 300$:
	
	$$
		**x** &= 10 e^{-300 / 50} [[ 99 ; 1 ; 100 ]] -10 e^{-2(300)} [[ 0 ; -49 ; 50 ]] - 500 e^{-300 / 25} [[ 0 ; 0 ; 1 ]]
		
		&\approx [[ 2.454 ; 0.025 ; 2.479 ]] + [[ 0 ; 0 ; 0 ]] + [[ 0 ; 0 ; -0.003 ]]
		
		&= [[ 2.454 ; 0.025 ; 2.482 ]].
	$$
	
	As we might expect, not much salt is left after so much time has passed.
	
###

### exc "$\text{TREE}(3)$"
	
	The life cycle of trees in a forest can be divided into three sections: when they're alive, when they've died, and when they've then decomposed into soil. Suppose $m_1(t)$, $m_2(t)$, and $m_3(t)$ give the mass of living trees, fallen trees, and soil from fallen trees in century $t$, respectively, and that they satisfy the following system of DEs:
	
	$$
		m_1' &= -3m_1
		
		m_2' &= 3m_1 - 2m_2
		
		m_3' &= 2m_2 - m_3.
	$$
	
	Find the general solution to the system.
	
###



### nav-buttons